{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wprowadzenie\n",
    "\n",
    "Zadania będą polegały na implementacji różnych wariantów algorytmu Q-Learning dla dwóch środowisk z biblioteki ~~OpenAI Gym~~ Gymnasium (https://gymnasium.farama.org/): `CartPole` i `LunarLander`.\n",
    "\n",
    "#### Uwaga: Możesz dowolnie modyfikować elementy tego notebooka (wstawiać komórki i zmieniać kod) o ile nie napisano gdzieś inaczej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zainstaluj bibliotekę Gymnasium i PyTorch\n",
    "!pip install \"gymnasium[box2d]==0.28\" torch matplotlib numpy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Środowisko `CartPole` i `LunarLander`\n",
    "\n",
    "Poniższy kod demonstruje działanie środowiska `CartPole` (https://gymnasium.farama.org/environments/classic_control/cart_pole/) i `LunarLander` (https://gymnasium.farama.org/environments/box2d/lunar_lander/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zobacz jak działają środowiska na przykładzie 100 kroków\n",
    "import gymnasium as gym\n",
    "\n",
    "#env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(100):\n",
    "  # Pokaż obraz z środowiska\n",
    "  env.render()\n",
    "\n",
    "  # Wybierz losową akcję z przestrzeni akcji\n",
    "  action = env.action_space.sample()\n",
    "\n",
    "  # Wykonaj akcję i otrzymaj informację o kolejnym stanie, nagrodzie\n",
    "  # i czy środowisko zostało zakończone.\n",
    "  # Zobacz https://gymnasium.farama.org/api/env/#gymnasium.Env.step\n",
    "  # by sprawdzić opis wszystkich zwracanych wartości.\n",
    "  observation, reward, term, trunc, _ = env.step(action)\n",
    "  done = term or trunc\n",
    "\n",
    "  if done:\n",
    "    observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zobacz jakim typem są obserwacje (reprezentacja stanu) oraz jaka jest przestrzeń akcji:\n",
    "print(\"Przestrzeń stanów:\", env.observation_space)\n",
    "print(\"Rozmiar wektora reprezntującego stany:\", env.observation_space.shape)\n",
    "print(\"Ostatnia obserwacja:\", type(observation), observation)\n",
    "print(\"Przestrzeń akcji:\", env.action_space)\n",
    "print(\"Ilość możliwych akcji:\", env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interface agenta\n",
    "\n",
    "Wszystkie implementacje będą w oparciu o klasę `Agent` przedstawioną poniżej. Powinna ona implementować dwie metody:\n",
    " 1. `process_transition`, która będzie wykorzystywana tylko podczas uczenia, by przekazać do agenta krotkę zawierającą obserwacje, podjętą akcję, nagrodę i następujący stan oraz informację czy jest on terminalny.\n",
    " 2. `get_action` która na podstawie obserwacji zwraca akcję, dodatkowy argument informuję Agenta czy jest w aktualnie podczas uczenia czy ewaluacji.\n",
    "\n",
    "Poniżej znajdziesz też funkcję `run_agent_in_env`, która korzysta z opisanego wyżej obiektu agenta w celu jego uczenia lub ewaluacji (podobną będziemy używać my podczas sprawdzania wszych rozwiązań). Możecie ją modyfikować jeśli widzicie taką potrzebę. Dopisaliśmy do tej funkcji rysowanie wykresu nagród z minionych epizodów, który uaktualnia się co ich zadaną liczbę."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "class Agent:\n",
    "  def __init__(self, env):\n",
    "    self.observation_space = env.observation_space\n",
    "    self.action_space = env.action_space\n",
    "\n",
    "  def process_transition(self, observation, action, reward, next_observation, done):\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def get_action(self, observation, learning):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "from IPython import display\n",
    "from tqdm.notebook import tqdm\n",
    "% matplotlib inline\n",
    "\n",
    "def plot_rewards(rewards):\n",
    "  plt.figure(figsize=(14, 6))\n",
    "  # good amount for plotting\n",
    "  plt.plot(rewards)\n",
    "  plt.plot([0, len(rewards)], [0, 0], 'k--')\n",
    "  plt.plot([0, len(rewards)], [500, 500], 'k--')\n",
    "  # min max across all time\n",
    "  plt.plot(pd.Series(rewards).expanding().min())\n",
    "  plt.plot(pd.Series(rewards).expanding().max())\n",
    "  # 50 % percentile\n",
    "  # moving average of every 250 episodes\n",
    "  plt.plot(pd.Series(rewards).rolling(250).quantile(0.5))\n",
    "\n",
    "  display.display(plt.gcf())\n",
    "  # display.clear_output(wait=True)\n",
    "\n",
    "\n",
    "def run_agent_in_env(env, agent, episodes, learning=False, plot=False, plot_interval=1000):\n",
    "  rewards = []\n",
    "  for episode in tqdm(range(episodes)):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    is_finished = False\n",
    "\n",
    "    while not is_finished:\n",
    "      action = agent.get_action(state, learning)\n",
    "\n",
    "      next_state, reward, is_terminated, is_truncated, _ = env.step(action)\n",
    "      is_finished = is_terminated or is_truncated\n",
    "      total_reward += reward\n",
    "\n",
    "      if learning: agent.process_transition(state, action, reward, next_state, is_finished)\n",
    "\n",
    "      state = next_state\n",
    "    rewards.append(total_reward)\n",
    "\n",
    "    # Wyświetl na wykresie nagrody otrzymane po kolei w epizodach\n",
    "    if plot and episode % plot_interval == 0:\n",
    "      plot_rewards(rewards)\n",
    "  return rewards"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Demonstracja na przykładzie losowego agenta\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "\n",
    "class RandomAgent(Agent):\n",
    "  def __init__(self, env):\n",
    "    super().__init__(env)\n",
    "\n",
    "  def process_transition(self, observation, action, reward, next_observation, done):\n",
    "    pass\n",
    "\n",
    "  def get_action(self, observation, learning):\n",
    "    return self.action_space.sample()\n",
    "\n",
    "\n",
    "random_agent = RandomAgent(env)\n",
    "rewards = run_agent_in_env(env, random_agent, 10000, plot=True, plot_interval=500)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zad. 3.1 - Tabularyczny Q-Learning (5 pkt.)\n",
    "\n",
    "W komórkach poniżej zaimplementuj prosty tabularyczny Q-Learning dla środowiska `CartPole`, korzystający podczas uczenia się z polityki epsilon-greedy. Ponieważ środowisko `CartPole` ma ciągłą przestrzeń stanów, musisz dokonać odpowiedniej dyskretyzacji stanów.\n",
    "\n",
    "Odpowiedź na pytanie: dlaczego po osiągnięciu dobrych wyników obserwujemy spadek jakości? Czy możemy temu jakoś zaradzić?\n",
    "\n",
    "Uwagi:\n",
    "- Dobra polityka dająca średni wynik powyżej 200, środowisko ma limit na 500 iteracji. Polityka dająca średni wynik blisko 500 jest więc bardzo dobra.\n",
    "- Uczenie nie powinno trwać więcej niż 10-15 minut na CPU.\n",
    "- Postaraj się dobrać i ustawić w `__init__` takie parametry by Twój Agent uczył się i działał jak najlepiej. Są one pod Twoją kontrolą w klasie, więc możesz je odpowiednio kontrolować w trakcie uczenia.\n",
    "\n",
    "\n",
    "#### Uwaga: nie zmieniaj nazwy klasy `TabularQLearningAgent`, ani pozycyjnych (tych bez wartości domyślnych) argumentów jej metod, możesz dopisywać argumenty z wartościami domyślnymi oraz oczywiście pomocnicze metody do klasy. Pomocnicze metody powinny być w tej samej komórce (sprawdzarka wyciągnie ze zgłoszonego notebooka wyłącznie komórkę z klasą `TabularQLearningAgent`, kod w innych komórkach nie będzie widziany przez sprawdzarkę!). Nie dopisuj do komórki z klasą innego kodu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Odpowiedzi:\n",
    "1. Po osiągnięciu dobrych wyników obserwujemy spadek jakości. Ponieważ agent zaczyna wybierać akcje, które nie są optymalne, ale dają mu większą szansę na eksplorację. Możemy temu zaradzić poprzez zmniejszenie wartości epsilon wraz z biegiem kolejnych epizodów, dzięki czemu agent będzie wybierał optymalne akcje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from gymnasium import Env\n",
    "\n",
    "class TabularQLearningAgent(Agent):\n",
    "  def __init__(self, env: Env):\n",
    "    super().__init__(env)\n",
    "    self.gamma = 0.999\n",
    "    self.epsilon = 0.5\n",
    "    self.eps_decay = 0.99\n",
    "    self.epsilon_min = 0.01\n",
    "    self.learning_rate = 0.25\n",
    "    self.bins = np.array([2, 6, 12, 6])\n",
    "    # based on https://gymnasium.farama.org/environments/classic_control/cart_pole/ info\n",
    "    self.bounds = np.array([4, 4, 0.4, 4])\n",
    "    self.limits = np.array([2, 2, 0.2, 2])\n",
    "\n",
    "    self.buckets = [\n",
    "      np.linspace(-limit, limit, bin)\n",
    "      for (limit, bin) in zip(self.limits, self.bins)\n",
    "    ]\n",
    "\n",
    "    self.quality = np.zeros((*self.bins, self.action_space.n))\n",
    "\n",
    "  def process_transition(self, observation, action, reward, next_observation, done):\n",
    "    state = self.discretize(observation)\n",
    "    next_state = self.discretize(next_observation)\n",
    "\n",
    "    prediction = self.quality[state][action]\n",
    "    if done:\n",
    "      target = reward\n",
    "    else:\n",
    "      target = reward + self.gamma * np.max(self.quality[next_state])\n",
    "\n",
    "    self.quality[state][action] -= self.learning_rate * (prediction - target)\n",
    "\n",
    "    if done: self.epsilon = max(self.epsilon * self.eps_decay, self.epsilon_min)\n",
    "\n",
    "  def get_action(self, observation, learning):\n",
    "    if learning and random.random() < self.epsilon: return self.action_space.sample()\n",
    "    state = self.discretize(observation)\n",
    "    return np.argmax(self.quality[state])\n",
    "\n",
    "  def discretize(self, state):\n",
    "    def indexer(index, value):\n",
    "      (min, max) = 0, self.bins[index] - 1\n",
    "      (limit, bound) = self.limits[index], self.bounds[index]\n",
    "      if value <= -limit: return min\n",
    "      if value >= limit: return max\n",
    "\n",
    "      offset = max * limit / bound\n",
    "      scale = max / bound\n",
    "      return int(np.round(scale * value - offset))\n",
    "    return tuple(map(indexer, range(len(state)), state))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "agent = TabularQLearningAgent(env)\n",
    "rewards = run_agent_in_env(env, agent, 10_000, learning=True, plot=True, plot_interval=500)\n",
    "print(f\"Reward: {np.mean(rewards)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Zad. 3.2 - Q-Learning z aproksymacją (25 pkt.)\n",
    "\n",
    "W komórkach poniżej zaimplementuj Q-Learning z aproksymacją dla środowiska `LunarLander`, korzystający podczas uczenia się z polityki epsilon-greedy, a jako aproksymatora użyj prostej sieć neuronową zaimplementowanej z wykorzystaniem biblioteki PyTroch. Zaimplementuj następujące ulepszenia algorytmu Q-Learning:\n",
    "- replay memory i uczenia batchowe zamiast uczenia online jak zrobiliśmy na zajęciach\n",
    "- network freezing\n",
    "- double q-learning\n",
    "\n",
    "Porównaj ze sobą różne warianty algorytmu (nie duplikuj swojej implementacji, dodaj odpowiednie argumenty w konstruktorze, które będą włączać/wyłączać odpowiednie rozszerzenie), zademonstruj oczywiście swoje wyniki pozostawiając odpowiedni output z komórek oraz je skomentuj. Opisz również, jak dokonałeś doboru parametrów Twojego modelu.\n",
    "\n",
    "Uwagi:\n",
    "- Dobra polityka powinna mieć średnią bliską 100, bardzo dobra bliską 200.\n",
    "- Uczenie nie powinno trwać więcej niż 10-15 minut (przy dobrze zaimplementowanych rozszerzeniach powinno być dużo szybsze) na CPU (możesz oczywiście użyć GPU jeśli masz taką możliwość, ale zadbaj, by domyślnie Twoja implementacja działała na CPU).\n",
    "- Postaraj się dobrać i ustawić w `__init__` takie parametry (w tym architekturę sieci neuronowej) by Twój Agent uczył się i działał jak najlepiej. Są one pod Twoją kontrolą w klasie, więc możesz je odpowiednio kontrolować w trakcie uczenia.\n",
    "- Założona architektura nie jest potrzebna do rozwiązania tego problemu, zacznij od prostego aproksymatora (np. liniowego albo sieci z dwoma warstwami) i stopniowo zwiększaj jego złożoność.\n",
    "- Możesz najpierw testować swoją implementację na środowisku `CartPole`, jest to prostszy problem i z dobrymi parametrami nawet prosta wersja Q-Learningu z aproksymacją powinna się go uczyć w nie więcej niż 2-3 minuty na CPU.\n",
    "\n",
    "#### Uwaga: nie zmieniaj nazwy klasy `NeuralQLearningAgent`, ani pozycyjnych (tych bez wartości domyślnych) argumentów jej metod, możesz dopisywać argumenty z wartościami domyślnymi oraz oczywiście pomocnicze metody do klasy. Pomocnicze metody powinny być w tej samej komórce (sprawdzarka wyciągnie ze zgłoszonego notebooka wyłącznie komórkę z klasą `NeuralQLearningAgent`, kod w innych komórkach nie będzie widziany przez sprawdzarkę!). Nie dopisuj do komórki z klasą innego kodu."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Odpowiedzi: Miejsce na Twoje odpowiedzi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class ModelModule(torch.nn.Module):\n",
    "  def __init__(self, input_size, output_size):\n",
    "    super().__init__()\n",
    "    self.model = torch.nn.Sequential(\n",
    "      torch.nn.Linear(input_size, 64),\n",
    "      torch.nn.ReLU(),\n",
    "      torch.nn.Linear(64, 64),\n",
    "      torch.nn.ReLU(),\n",
    "      torch.nn.Linear(64, output_size)\n",
    "    )\n",
    "\n",
    "  def forward(self, x): return self.model(x)\n",
    "\n",
    "  def predict(self, x, *, use_grad=False):\n",
    "    x = torch.Tensor(x)\n",
    "\n",
    "    if use_grad:\n",
    "      return self(x)\n",
    "    with torch.no_grad():\n",
    "      return self(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "class MemoryModule(object):\n",
    "  def __init__(self, capacity: int = None):\n",
    "    super().__init__()\n",
    "    self.capacity = capacity\n",
    "    self.contents = []\n",
    "\n",
    "  def update(self, item):\n",
    "    self.contents.append(item)\n",
    "    if self.capacity and len(self.contents) > self.capacity: self.contents.pop(0)\n",
    "\n",
    "  def sample(self, count: int):\n",
    "    return random.choices(self.contents, k=count)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.contents)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [
    "from typing import Iterable, Tuple\n",
    "import numpy as np\n",
    "\n",
    "Experience = Tuple[np.ndarray, int, float, np.ndarray, bool]\n",
    "class NeuralQLearningAgent(Agent):\n",
    "  def get_action(self, observation, learning):\n",
    "    if learning and np.random.random() < self.epsilon:  return self.action_space.sample()\n",
    "    return torch.argmax(self.model.predict(observation)).item()\n",
    "\n",
    "  def __init__(self, env, use_memory=None, use_freeze=None, use_double=None):\n",
    "    super().__init__(env)\n",
    "    def create_model():\n",
    "      return ModelModule(env.observation_space.shape[0], env.action_space.n)\n",
    "    self.model = create_model()\n",
    "    self.optimizer = torch.optim.RMSprop(self.model.parameters(), lr=0.001)\n",
    "    self.loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    self.uses_memory = bool(use_memory)\n",
    "    if self.uses_memory:\n",
    "      self.memory = MemoryModule(use_memory.capacity)\n",
    "      self.memory_iterations = use_memory.iterations\n",
    "      self.batch_size = use_memory.batch_size\n",
    "\n",
    "    self.uses_double = bool(use_double)\n",
    "    if self.uses_double:\n",
    "      self.model_2 = create_model()\n",
    "\n",
    "\n",
    "    self.iteration = 0\n",
    "    self.gamma = 0.99\n",
    "    self.epsilon = 0.2\n",
    "    self.epsilon_decay = 0.99\n",
    "    self.epsilon_min = 0.01\n",
    "\n",
    "  def error(self, experience: Experience):\n",
    "    (observation, action, reward, next_observation, done) = experience\n",
    "\n",
    "    state = torch.Tensor(observation)\n",
    "    next_state = torch.Tensor(next_observation)\n",
    "    action = torch.LongTensor([action])\n",
    "    reward = torch.Tensor([reward])\n",
    "\n",
    "    predicted = self.model(state)[action]\n",
    "    target = reward if done else reward + self.gamma * torch.max(self.model(next_state))\n",
    "\n",
    "    return self.loss_fn(predicted, target)\n",
    "\n",
    "  def train(self, experiences: Iterable[Experience]):\n",
    "    for experience in experiences:\n",
    "      self.optimizer.zero_grad()\n",
    "      loss = self.error(experience)\n",
    "      loss.backward()\n",
    "      self.optimizer.step()\n",
    "\n",
    "  def update_epsilon(self): self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "  @property\n",
    "  def should_train_batch(self):\n",
    "    return self.iteration % self.memory_iterations == 0 and len(self.memory) >= self.batch_size\n",
    "\n",
    "  def process_transition(self, observation, action, reward, next_observation, done):\n",
    "    experience: Experience = (observation, action, reward, next_observation, done)\n",
    "\n",
    "    self.iteration += 1\n",
    "    if self.uses_memory:\n",
    "      self.memory.update(experience)\n",
    "\n",
    "      if self.should_train_batch:\n",
    "        self.train(self.memory.sample(self.batch_size))\n",
    "    else:\n",
    "      self.train([experience])\n",
    "\n",
    "    if done: self.update_epsilon()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check gpu\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# check if used\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Uczenie/testowania agenta\n",
    "class Item(dict):\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "agent = NeuralQLearningAgent(env, use_memory=Item(iterations=10, batch_size=32, capacity=50))\n",
    "rewards = run_agent_in_env(env, agent, 10_000, learning=True, plot=True, plot_interval=100)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9cf6ace87ce140ee73c3212e83cfef8944681e1010745419fbed842180d29b9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
