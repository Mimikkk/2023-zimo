{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zaawansowane Metody Inteligencji Obliczeniowej\n",
    "# Zadanie domowe 3 (30 pkt.)\n",
    "### Prowadzący: Michał Kempka, Marek Wydmuch\n",
    "### Autor: twoje imię i nazwisko + numer indeksu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wprowadzenie\n",
    "\n",
    "Zadania będą polegały na implementacji różnych wariantów algorytmu Q-Learning dla dwóch środowisk z biblioteki ~~OpenAI Gym~~ Gymnasium (https://gymnasium.farama.org/): `CartPole` i `LunarLander`.\n",
    "\n",
    "#### Uwaga: Możesz dowolnie modyfikować elementy tego notebooka (wstawiać komórki i zmieniać kod) o ile nie napisano gdzieś inaczej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zainstaluj bibliotekę Gymnasium i PyTorch\n",
    "!pip install \"gymnasium[box2d]==0.28\" torch matplotlib numpy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Środowisko `CartPole` i `LunarLander`\n",
    "\n",
    "Poniższy kod demonstruje działanie środowiska `CartPole` (https://gymnasium.farama.org/environments/classic_control/cart_pole/) i `LunarLander` (https://gymnasium.farama.org/environments/box2d/lunar_lander/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zobacz jak działają środowiska na przykładzie 100 kroków\n",
    "import gymnasium as gym\n",
    "\n",
    "#env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(100):\n",
    "    # Pokaż obraz z środowiska\n",
    "    env.render()\n",
    "\n",
    "    # Wybierz losową akcję z przestrzeni akcji\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    # Wykonaj akcję i otrzymaj informację o kolejnym stanie, nagrodzie \n",
    "    # i czy środowisko zostało zakończone.\n",
    "    # Zobacz https://gymnasium.farama.org/api/env/#gymnasium.Env.step \n",
    "    # by sprawdzić opis wszystkich zwracanych wartości.\n",
    "    observation, reward, term, trunc, _ = env.step(action)\n",
    "    done = term or trunc\n",
    "    \n",
    "    if done:\n",
    "        observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zobacz jakim typem są obserwacje (reprezentacja stanu) oraz jaka jest przestrzeń akcji:\n",
    "print(\"Przestrzeń stanów:\", env.observation_space)\n",
    "print(\"Rozmiar wektora reprezntującego stany:\", env.observation_space.shape)\n",
    "print(\"Ostatnia obserwacja:\", type(observation), observation)\n",
    "print(\"Przestrzeń akcji:\", env.action_space)\n",
    "print(\"Ilość możliwych akcji:\", env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interface agenta\n",
    "\n",
    "Wszystkie implementacje będą w oparciu o klasę `Agent` przedstawioną poniżej. Powinna ona implementować dwie metody: 1) `process_transition`, która będzie wykorzystywana tylko podczas uczenia by przekazać do agenta krotkę zawierającą obserwacje, podjętą akcję, nagrodę i następujący stan oraz informację czy jest on terminalny. 2) `get_action` która na podstawie obserwacji zwraca akcję, dodatkowy argument informuję Agenta czy jest w aktualnie podczas uczenia czy ewaluacji.\n",
    "\n",
    "Poniżej znajdziiesz też funkcję `run_agent_in_env`, która korzysta z opisanego wyżej obiektu agenta w celu jego uczenia lub ewaluacji (podobną będziemy używać my podczas sprawdzania wszych rozwiązań). Możecie ją modyfikować jeśli widzicie taką potrzebę. Dopisaliśmy do tej funkcji rysowanie wykresu nagród z minionych epizodów, który uaktualnia się co ich zadaną liczbę."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        self.observation_space = env.observation_space\n",
    "        self.action_space = env.action_space\n",
    "    \n",
    "    def process_transition(self, observation, action, reward, next_observation, done):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def get_action(self, observation, learning):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def plot_rewards(rewards):\n",
    "    plt.figure(figsize=(14,6))\n",
    "    plt.plot(rewards)\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    \n",
    "def run_agent_in_env(env, agent, episodes, learning=False, plot=False, plot_interval=1000):\n",
    "    rewards = []\n",
    "    for episode in range(episodes):\n",
    "        observation, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done :\n",
    "            # Zapytajmy agenta o akcje dla aktualnego stanu\n",
    "            action = agent.get_action(observation, learning)\n",
    "            \n",
    "            # Wykonajmy akcje\n",
    "            next_observation, reward, term, trunc, _ = env.step(action)\n",
    "            done = term or trunc\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Jeśli się uczymy, przekażmy przejście do agenta\n",
    "            if learning:\n",
    "                agent.process_transition(observation, action, reward, next_observation, done)\n",
    "            \n",
    "            observation = next_observation\n",
    "        rewards.append(total_reward)\n",
    "        \n",
    "        # Wyświetl na wykresie nagrody otrzymane po kolei w epizodach\n",
    "        if plot and episode % plot_interval == 0:\n",
    "            plot_rewards(rewards)\n",
    "    return rewards    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstracja na przykładzie losowego agenta\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "class RandomAgent(Agent):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def process_transition(self, observation, action, reward, next_observation, done):\n",
    "        pass\n",
    "        \n",
    "    def get_action(self, observation, learning):\n",
    "        return self.action_space.sample()\n",
    "    \n",
    "random_agent = RandomAgent(env)\n",
    "rewards = run_agent_in_env(env, random_agent, 1000, plot=True, plot_interval=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zad. 3.1 - Tabularyczny Q-Learning (5 pkt.)\n",
    "\n",
    "W komórkach poniżej zaimplementuj prosty tabularyczny Q-Learning dla środowiska `CartPole`, korzystający podczas uczenia się z polityki epsilon-greedy. Ponieważ środowisko `CartPole` ma ciągłą przestrzeń stanów, musisz dokonać odpowiedniej dyskretyzacji stanów.\n",
    "\n",
    "Odpowiedź na pytanie: dlaczego po osiągnięciu dobrych wyników obserwujemy spadek jakości? Czy możemy temu jakoś zaradzić?\n",
    "\n",
    "Uwagi:\n",
    "- Dobra polityka dająca średni wynik powyżej 200, środowisko ma limit na 500 iteracji. Polityka dająca średni wynik blisko 500 jest więc bardzo dobra.\n",
    "- Uczenie nie powinno trwać więcej niż 10-15 minut na CPU.\n",
    "- Postaraj się dobrać i ustawić w `__init__` takie parametry by Twój Agent uczył się i działał jak najlepiej. Są one pod Twoją kontrolą w klasie, więc możesz je odpowiednio kontrolować w trakcie uczenia.\n",
    "\n",
    "\n",
    "#### Uwaga: nie zmieniaj nazwy klasy `TabularQLearningAgent`, ani pozycyjnych (tych bez wartości domyślnych) argumentów jej metod, możesz dopisywać argumenty z wartościami domyślnymi oraz oczywiście pomocnicze metody do klasy. Pomocnicze metody powinny być w tej samej komórce (sprawdzarka wyciągnie ze zgłoszonego notebooka wyłącznie komórkę z klasą `TabularQLearningAgent`, kod w innych komórkach nie będzie widziany przez sprawdzarkę!). Nie dopisuj do komórki z klasą innego kodu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Odpowiedzi: Miejsce na Twoje odpowiedzi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularQLearningAgent(Agent):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        # Zainicjalizuj tutaj swojego agenta\n",
    "    \n",
    "    def process_transition(self, observation, action, reward, next_observation, done):\n",
    "        # Miejsce na Twoją implementację\n",
    "        \n",
    "    def get_action(self, observation, learning):\n",
    "        # Miejsce na Twoją implementację"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uczenie/testowania agenta\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "agent = TabularQLearningAgent(env)\n",
    "rewards = run_agent_in_env(env, agent, 10000, learning=True, plot=True, plot_interval=250)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zad. 3.2 - Q-Learning z aproksymacją (25 pkt.)\n",
    "\n",
    "W komórkach poniżej zaimplementuj Q-Learning z aproksymacją dla środowiska `LunarLander`, korzystający podczas uczenia się z polityki epsilon-greedy, a jako aproksymatora użyj prostej sieć neuronową zaimplementowanej z wykorzystaniem biblioteki PyTroch. Zaimplementuj następujące ulepszenia algorytmu Q-Learning:\n",
    "- replay memory i uczenia batchowe zamiast uczenia online jak zrobiliśmy na zajęciach\n",
    "- network freezing\n",
    "- double q-learning\n",
    "\n",
    "Porównaj ze sobą różne warianty algorytmu (nie duplikuj swojej implementacji, dodaj odpowiednie argumenty w konstruktorze, które będą włączać/wyłączać odpowiednie rozszerzenie), zademonstruj oczywiście swoje wyniki pozostawiając odpowiedni output z komórek oraz je skomentuj. Opisz również, jak dokonałeś doboru parametrów Twojego modelu.\n",
    "\n",
    "Uwagi:\n",
    "- Dobra polityka powinna mieć średnią bliską 100, bardzo dobra bliską 200.\n",
    "- Uczenie nie powinno trwać więcej niż 10-15 minut (przy dobrze zaimplementowanych rozszerzeniach powinno być dużo szybsze) na CPU (możesz oczywiście użyć GPU jeśli masz taką możliwość, ale zadbaj, by domyślnie Twoja implementacja działała na CPU).\n",
    "- Postaraj się dobrać i ustawić w `__init__` takie parametry (w tym architekturę sieci neuronowej) by Twój Agent uczył się i działał jak najlepiej. Są one pod Twoją kontrolą w klasie, więc możesz je odpowiednio kontrolować w trakcie uczenia.\n",
    "- Założona architektura nie jest potrzebna do rozwiązania tego problemu, zacznij od prostego aproksymatora (np. liniowego albo sieci z dwoma warstwami) i stopniowo zwiększaj jego złożoność.\n",
    "- Możesz najpierw testować swoją implementację na środowisku `CartPole`, jest to prostszy problem i z dobrymi parametrami nawet prosta wersja Q-Learningu z aproksymacją powinna się go uczyć w nie więcej niż 2-3 minuty na CPU.\n",
    "\n",
    "#### Uwaga: nie zmieniaj nazwy klasy `NeuralQLearningAgent`, ani pozycyjnych (tych bez wartości domyślnych) argumentów jej metod, możesz dopisywać argumenty z wartościami domyślnymi oraz oczywiście pomocnicze metody do klasy. Pomocnicze metody powinny być w tej samej komórce (sprawdzarka wyciągnie ze zgłoszonego notebooka wyłącznie komórkę z klasą `NeuralQLearningAgent`, kod w innych komórkach nie będzie widziany przez sprawdzarkę!). Nie dopisuj do komórki z klasą innego kodu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Odpowiedzi: Miejsce na Twoje odpowiedzi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralQLearningAgent(Agent):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        # Zainicjalizuj tutaj swojego agenta\n",
    "    \n",
    "    def process_transition(self, observation, action, reward, next_observation, done):\n",
    "        # Miejsce na Twoją implementację\n",
    "        \n",
    "    def get_action(self, observation, learning):\n",
    "        # Miejsce na Twoją implementację"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uczenie/testowania agenta\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "agent = NeuralQLearningAgent(env)\n",
    "rewards = run_agent_in_env(env, agent, 10000, learning=True, plot=True, plot_interval=250)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9cf6ace87ce140ee73c3212e83cfef8944681e1010745419fbed842180d29b9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
