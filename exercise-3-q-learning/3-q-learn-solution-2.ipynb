{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wprowadzenie\n",
    "\n",
    "Zadania będą polegały na implementacji różnych wariantów algorytmu Q-Learning dla dwóch środowisk z biblioteki ~~OpenAI Gym~~ Gymnasium (https://gymnasium.farama.org/): `CartPole` i `LunarLander`.\n",
    "\n",
    "#### Uwaga: Możesz dowolnie modyfikować elementy tego notebooka (wstawiać komórki i zmieniać kod) o ile nie napisano gdzieś inaczej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zainstaluj bibliotekę Gymnasium i PyTorch\n",
    "!pip install \"gymnasium[box2d]==0.28\" torch matplotlib numpy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Środowisko `CartPole` i `LunarLander`\n",
    "\n",
    "Poniższy kod demonstruje działanie środowiska `CartPole` (https://gymnasium.farama.org/environments/classic_control/cart_pole/) i `LunarLander` (https://gymnasium.farama.org/environments/box2d/lunar_lander/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "DependencyNotInstalled",
     "evalue": "Box2D is not installed, run `pip install gymnasium[box2d]`",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "File \u001B[0;32m~/.local/lib/python3.11/site-packages/gymnasium/envs/box2d/bipedal_walker.py:15\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 15\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mBox2D\u001B[39;00m\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mBox2D\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mb2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     17\u001B[0m         circleShape,\n\u001B[1;32m     18\u001B[0m         contactListener,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     22\u001B[0m         revoluteJointDef,\n\u001B[1;32m     23\u001B[0m     )\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'Box2D'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mDependencyNotInstalled\u001B[0m                    Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mgymnasium\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mgym\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m#env = gym.make(\"CartPole-v1\", render_mode=\"human\")\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m env \u001B[38;5;241m=\u001B[39m \u001B[43mgym\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmake\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mLunarLander-v2\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrender_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mhuman\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m observation, info \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mreset()\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m100\u001B[39m):\n\u001B[1;32m      9\u001B[0m   \u001B[38;5;66;03m# Pokaż obraz z środowiska\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.11/site-packages/gymnasium/envs/registration.py:755\u001B[0m, in \u001B[0;36mmake\u001B[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001B[0m\n\u001B[1;32m    752\u001B[0m     env_creator \u001B[38;5;241m=\u001B[39m env_spec\u001B[38;5;241m.\u001B[39mentry_point\n\u001B[1;32m    753\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    754\u001B[0m     \u001B[38;5;66;03m# Assume it's a string\u001B[39;00m\n\u001B[0;32m--> 755\u001B[0m     env_creator \u001B[38;5;241m=\u001B[39m \u001B[43mload_env_creator\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv_spec\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mentry_point\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    757\u001B[0m \u001B[38;5;66;03m# Determine if to use the rendering\u001B[39;00m\n\u001B[1;32m    758\u001B[0m render_modes: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.11/site-packages/gymnasium/envs/registration.py:553\u001B[0m, in \u001B[0;36mload_env_creator\u001B[0;34m(name)\u001B[0m\n\u001B[1;32m    544\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Loads an environment with name of style ``\"(import path):(environment name)\"`` and returns the environment creation function, normally the environment class type.\u001B[39;00m\n\u001B[1;32m    545\u001B[0m \n\u001B[1;32m    546\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    550\u001B[0m \u001B[38;5;124;03m    The environment constructor for the given environment name.\u001B[39;00m\n\u001B[1;32m    551\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    552\u001B[0m mod_name, attr_name \u001B[38;5;241m=\u001B[39m name\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 553\u001B[0m mod \u001B[38;5;241m=\u001B[39m \u001B[43mimportlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimport_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmod_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    554\u001B[0m fn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(mod, attr_name)\n\u001B[1;32m    555\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m fn\n",
      "File \u001B[0;32m/usr/lib64/python3.11/importlib/__init__.py:126\u001B[0m, in \u001B[0;36mimport_module\u001B[0;34m(name, package)\u001B[0m\n\u001B[1;32m    124\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m    125\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m--> 126\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1206\u001B[0m, in \u001B[0;36m_gcd_import\u001B[0;34m(name, package, level)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1178\u001B[0m, in \u001B[0;36m_find_and_load\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1128\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:241\u001B[0m, in \u001B[0;36m_call_with_frames_removed\u001B[0;34m(f, *args, **kwds)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1206\u001B[0m, in \u001B[0;36m_gcd_import\u001B[0;34m(name, package, level)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1178\u001B[0m, in \u001B[0;36m_find_and_load\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1149\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:690\u001B[0m, in \u001B[0;36m_load_unlocked\u001B[0;34m(spec)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap_external>:940\u001B[0m, in \u001B[0;36mexec_module\u001B[0;34m(self, module)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:241\u001B[0m, in \u001B[0;36m_call_with_frames_removed\u001B[0;34m(f, *args, **kwds)\u001B[0m\n",
      "File \u001B[0;32m~/.local/lib/python3.11/site-packages/gymnasium/envs/box2d/__init__.py:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgymnasium\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01menvs\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbox2d\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbipedal_walker\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BipedalWalker, BipedalWalkerHardcore\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgymnasium\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01menvs\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbox2d\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcar_racing\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CarRacing\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgymnasium\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01menvs\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbox2d\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlunar_lander\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LunarLander, LunarLanderContinuous\n",
      "File \u001B[0;32m~/.local/lib/python3.11/site-packages/gymnasium/envs/box2d/bipedal_walker.py:25\u001B[0m\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mBox2D\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mb2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     17\u001B[0m         circleShape,\n\u001B[1;32m     18\u001B[0m         contactListener,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     22\u001B[0m         revoluteJointDef,\n\u001B[1;32m     23\u001B[0m     )\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m---> 25\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m DependencyNotInstalled(\n\u001B[1;32m     26\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBox2D is not installed, run `pip install gymnasium[box2d]`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     27\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m TYPE_CHECKING:\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpygame\u001B[39;00m\n",
      "\u001B[0;31mDependencyNotInstalled\u001B[0m: Box2D is not installed, run `pip install gymnasium[box2d]`"
     ]
    }
   ],
   "source": [
    "# Zobacz jak działają środowiska na przykładzie 100 kroków\n",
    "import gymnasium as gym\n",
    "\n",
    "#env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(100):\n",
    "  # Pokaż obraz z środowiska\n",
    "  env.render()\n",
    "\n",
    "  # Wybierz losową akcję z przestrzeni akcji\n",
    "  action = env.action_space.sample()\n",
    "\n",
    "  # Wykonaj akcję i otrzymaj informację o kolejnym stanie, nagrodzie\n",
    "  # i czy środowisko zostało zakończone.\n",
    "  # Zobacz https://gymnasium.farama.org/api/env/#gymnasium.Env.step\n",
    "  # by sprawdzić opis wszystkich zwracanych wartości.\n",
    "  observation, reward, term, trunc, _ = env.step(action)\n",
    "  done = term or trunc\n",
    "\n",
    "  if done:\n",
    "    observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zobacz jakim typem są obserwacje (reprezentacja stanu) oraz jaka jest przestrzeń akcji:\n",
    "print(\"Przestrzeń stanów:\", env.observation_space)\n",
    "print(\"Rozmiar wektora reprezntującego stany:\", env.observation_space.shape)\n",
    "print(\"Ostatnia obserwacja:\", type(observation), observation)\n",
    "print(\"Przestrzeń akcji:\", env.action_space)\n",
    "print(\"Ilość możliwych akcji:\", env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interface agenta\n",
    "\n",
    "Wszystkie implementacje będą w oparciu o klasę `Agent` przedstawioną poniżej. Powinna ona implementować dwie metody:\n",
    " 1. `process_transition`, która będzie wykorzystywana tylko podczas uczenia, by przekazać do agenta krotkę zawierającą obserwacje, podjętą akcję, nagrodę i następujący stan oraz informację czy jest on terminalny.\n",
    " 2. `get_action` która na podstawie obserwacji zwraca akcję, dodatkowy argument informuję Agenta czy jest w aktualnie podczas uczenia czy ewaluacji.\n",
    "\n",
    "Poniżej znajdziesz też funkcję `run_agent_in_env`, która korzysta z opisanego wyżej obiektu agenta w celu jego uczenia lub ewaluacji (podobną będziemy używać my podczas sprawdzania wszych rozwiązań). Możecie ją modyfikować jeśli widzicie taką potrzebę. Dopisaliśmy do tej funkcji rysowanie wykresu nagród z minionych epizodów, który uaktualnia się co ich zadaną liczbę."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "class Agent:\n",
    "  def __init__(self, env):\n",
    "    self.observation_space = env.observation_space\n",
    "    self.action_space = env.action_space\n",
    "\n",
    "  def process_transition(self, observation, action, reward, next_observation, done):\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def get_action(self, observation, learning):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "from IPython import display\n",
    "from tqdm.notebook import tqdm\n",
    "% matplotlib inline\n",
    "\n",
    "def plot_rewards(rewards):\n",
    "  plt.figure(figsize=(14, 6))\n",
    "  # good amount for plotting\n",
    "  plt.plot(rewards)\n",
    "  plt.plot([0, len(rewards)], [0, 0], 'k--')\n",
    "  plt.plot([0, len(rewards)], [500, 500], 'k--')\n",
    "  # min max across all time\n",
    "  plt.plot(pd.Series(rewards).expanding().min())\n",
    "  plt.plot(pd.Series(rewards).expanding().max())\n",
    "  # 50 % percentile\n",
    "  # moving average of every 250 episodes\n",
    "  plt.plot(pd.Series(rewards).rolling(250).quantile(0.5))\n",
    "\n",
    "  display.display(plt.gcf())\n",
    "  # display.clear_output(wait=True)\n",
    "\n",
    "\n",
    "def run_agent_in_env(env, agent, episodes, learning=False, plot=False, plot_interval=1000):\n",
    "  rewards = []\n",
    "  for episode in tqdm(range(episodes)):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    is_finished = False\n",
    "\n",
    "    while not is_finished:\n",
    "      action = agent.get_action(state, learning)\n",
    "\n",
    "      next_state, reward, is_terminated, is_truncated, _ = env.step(action)\n",
    "      is_finished = is_terminated or is_truncated\n",
    "      total_reward += reward\n",
    "\n",
    "      if learning: agent.process_transition(state, action, reward, next_state, is_finished)\n",
    "\n",
    "      state = next_state\n",
    "    rewards.append(total_reward)\n",
    "\n",
    "    # Wyświetl na wykresie nagrody otrzymane po kolei w epizodach\n",
    "    if plot and episode > 0 and episode % plot_interval == 0:\n",
    "      plot_rewards(rewards)\n",
    "  return rewards"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Demonstracja na przykładzie losowego agenta\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "\n",
    "class RandomAgent(Agent):\n",
    "  def __init__(self, env):\n",
    "    super().__init__(env)\n",
    "\n",
    "  def process_transition(self, observation, action, reward, next_observation, done):\n",
    "    pass\n",
    "\n",
    "  def get_action(self, observation, learning):\n",
    "    return self.action_space.sample()\n",
    "\n",
    "\n",
    "random_agent = RandomAgent(env)\n",
    "rewards = run_agent_in_env(env, random_agent, 10000, plot=True, plot_interval=500)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zad. 3.1 - Tabularyczny Q-Learning (5 pkt.)\n",
    "\n",
    "W komórkach poniżej zaimplementuj prosty tabularyczny Q-Learning dla środowiska `CartPole`, korzystający podczas uczenia się z polityki epsilon-greedy. Ponieważ środowisko `CartPole` ma ciągłą przestrzeń stanów, musisz dokonać odpowiedniej dyskretyzacji stanów.\n",
    "\n",
    "Odpowiedź na pytanie: dlaczego po osiągnięciu dobrych wyników obserwujemy spadek jakości? Czy możemy temu jakoś zaradzić?\n",
    "\n",
    "Uwagi:\n",
    "- Dobra polityka dająca średni wynik powyżej 200, środowisko ma limit na 500 iteracji. Polityka dająca średni wynik blisko 500 jest więc bardzo dobra.\n",
    "- Uczenie nie powinno trwać więcej niż 10-15 minut na CPU.\n",
    "- Postaraj się dobrać i ustawić w `__init__` takie parametry by Twój Agent uczył się i działał jak najlepiej. Są one pod Twoją kontrolą w klasie, więc możesz je odpowiednio kontrolować w trakcie uczenia.\n",
    "\n",
    "\n",
    "#### Uwaga: nie zmieniaj nazwy klasy `TabularQLearningAgent`, ani pozycyjnych (tych bez wartości domyślnych) argumentów jej metod, możesz dopisywać argumenty z wartościami domyślnymi oraz oczywiście pomocnicze metody do klasy. Pomocnicze metody powinny być w tej samej komórce (sprawdzarka wyciągnie ze zgłoszonego notebooka wyłącznie komórkę z klasą `TabularQLearningAgent`, kod w innych komórkach nie będzie widziany przez sprawdzarkę!). Nie dopisuj do komórki z klasą innego kodu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Odpowiedzi:\n",
    "1. Po osiągnięciu dobrych wyników obserwujemy spadek jakości. Ponieważ agent zaczyna wybierać akcje, które nie są optymalne, ale dają mu większą szansę na eksplorację. Możemy temu zaradzić poprzez zmniejszenie wartości epsilon wraz z biegiem kolejnych epizodów, dzięki czemu agent będzie wybierał optymalne akcje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from gymnasium import Env\n",
    "\n",
    "\n",
    "class TabularQLearningAgent(Agent):\n",
    "  def __init__(self, env: Env):\n",
    "    super().__init__(env)\n",
    "    self.gamma = 0.999\n",
    "    self.epsilon = 0.5\n",
    "    self.eps_decay = 0.99\n",
    "    self.epsilon_min = 0.01\n",
    "    self.learning_rate = 0.25\n",
    "    self.bins = np.array([2, 6, 12, 6])\n",
    "    # based on https://gymnasium.farama.org/environments/classic_control/cart_pole/ info\n",
    "    self.bounds = np.array([4, 4, 0.4, 4])\n",
    "    self.limits = np.array([2, 2, 0.2, 2])\n",
    "\n",
    "    self.buckets = [\n",
    "      np.linspace(-limit, limit, bin)\n",
    "      for (limit, bin) in zip(self.limits, self.bins)\n",
    "    ]\n",
    "\n",
    "    self.quality = np.zeros((*self.bins, self.action_space.n))\n",
    "\n",
    "  def process_transition(self, observation, action, reward, next_observation, done):\n",
    "    state = self.discretize(observation)\n",
    "    next_state = self.discretize(next_observation)\n",
    "\n",
    "    prediction = self.quality[state][action]\n",
    "    if done:\n",
    "      target = reward\n",
    "    else:\n",
    "      target = reward + self.gamma * np.max(self.quality[next_state])\n",
    "\n",
    "    self.quality[state][action] -= self.learning_rate * (prediction - target)\n",
    "\n",
    "    if done: self.epsilon = max(self.epsilon * self.eps_decay, self.epsilon_min)\n",
    "\n",
    "  def get_action(self, observation, learning):\n",
    "    if learning and random.random() < self.epsilon: return self.action_space.sample()\n",
    "    state = self.discretize(observation)\n",
    "    return np.argmax(self.quality[state])\n",
    "\n",
    "  def discretize(self, state):\n",
    "    def indexer(index, value):\n",
    "      (min, max) = 0, self.bins[index] - 1\n",
    "      (limit, bound) = self.limits[index], self.bounds[index]\n",
    "      if value <= -limit: return min\n",
    "      if value >= limit: return max\n",
    "\n",
    "      offset = max * limit / bound\n",
    "      scale = max / bound\n",
    "      return int(np.round(scale * value - offset))\n",
    "\n",
    "    return tuple(map(indexer, range(len(state)), state))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "agent = TabularQLearningAgent(env)\n",
    "rewards = run_agent_in_env(env, agent, 10_000, learning=True, plot=True, plot_interval=500)\n",
    "print(f\"Reward: {np.mean(rewards)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Zad. 3.2 - Q-Learning z aproksymacją (25 pkt.)\n",
    "\n",
    "W komórkach poniżej zaimplementuj Q-Learning z aproksymacją dla środowiska `LunarLander`, korzystający podczas uczenia się z polityki epsilon-greedy, a jako aproksymatora użyj prostej sieć neuronową zaimplementowanej z wykorzystaniem biblioteki PyTroch. Zaimplementuj następujące ulepszenia algorytmu Q-Learning:\n",
    "- replay memory i uczenia batchowe zamiast uczenia online jak zrobiliśmy na zajęciach\n",
    "- network freezing\n",
    "- double q-learning\n",
    "\n",
    "Porównaj ze sobą różne warianty algorytmu (nie duplikuj swojej implementacji, dodaj odpowiednie argumenty w konstruktorze, które będą włączać/wyłączać odpowiednie rozszerzenie), zademonstruj oczywiście swoje wyniki pozostawiając odpowiedni output z komórek oraz je skomentuj. Opisz również, jak dokonałeś doboru parametrów Twojego modelu.\n",
    "\n",
    "Uwagi:\n",
    "- Dobra polityka powinna mieć średnią bliską 100, bardzo dobra bliską 200.\n",
    "- Uczenie nie powinno trwać więcej niż 10-15 minut (przy dobrze zaimplementowanych rozszerzeniach powinno być dużo szybsze) na CPU (możesz oczywiście użyć GPU jeśli masz taką możliwość, ale zadbaj, by domyślnie Twoja implementacja działała na CPU).\n",
    "- Postaraj się dobrać i ustawić w `__init__` takie parametry (w tym architekturę sieci neuronowej) by Twój Agent uczył się i działał jak najlepiej. Są one pod Twoją kontrolą w klasie, więc możesz je odpowiednio kontrolować w trakcie uczenia.\n",
    "- Założona architektura nie jest potrzebna do rozwiązania tego problemu, zacznij od prostego aproksymatora (np. liniowego albo sieci z dwoma warstwami) i stopniowo zwiększaj jego złożoność.\n",
    "- Możesz najpierw testować swoją implementację na środowisku `CartPole`, jest to prostszy problem i z dobrymi parametrami nawet prosta wersja Q-Learningu z aproksymacją powinna się go uczyć w nie więcej niż 2-3 minuty na CPU.\n",
    "\n",
    "#### Uwaga: nie zmieniaj nazwy klasy `NeuralQLearningAgent`, ani pozycyjnych (tych bez wartości domyślnych) argumentów jej metod, możesz dopisywać argumenty z wartościami domyślnymi oraz oczywiście pomocnicze metody do klasy. Pomocnicze metody powinny być w tej samej komórce (sprawdzarka wyciągnie ze zgłoszonego notebooka wyłącznie komórkę z klasą `NeuralQLearningAgent`, kod w innych komórkach nie będzie widziany przez sprawdzarkę!). Nie dopisuj do komórki z klasą innego kodu."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Odpowiedzi: Miejsce na Twoje odpowiedzi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from typing import List\n",
    "from random import choices\n",
    "from gymnasium import Env\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class Configuration(dict):\n",
    "  __getattr__ = dict.get\n",
    "  __setattr__ = dict.__setitem__\n",
    "  __delattr__ = dict.__delitem__\n",
    "\n",
    "\n",
    "Experience = namedtuple(\"Experience\", field_names=[\n",
    "  \"state\",\n",
    "  \"action\",\n",
    "  \"reward\",\n",
    "  \"next_state\",\n",
    "  \"done\"\n",
    "])\n",
    "\n",
    "\n",
    "class MemoryModule(object):\n",
    "  def __init__(self, capacity: int = None) -> None:\n",
    "    self.capacity = capacity\n",
    "    self.contents = []\n",
    "\n",
    "  def __len__(self) -> int:\n",
    "    return len(self.contents)\n",
    "\n",
    "  def append(self, experience: Experience) -> None:\n",
    "    if self.capacity and len(self) >= self.capacity: self.contents.pop(0)\n",
    "    self.contents.append(experience)\n",
    "\n",
    "  def sample(self, count: int) -> List[Experience]:\n",
    "    return choices(self.contents, k=count)\n",
    "\n",
    "\n",
    "class ModelModule(torch.nn.Module):\n",
    "  def __init__(self, input_size: int, hidden_sizes: List[int], output_size: int):\n",
    "    super().__init__()\n",
    "\n",
    "    self.model = torch.nn.Sequential(\n",
    "      torch.nn.Linear(input_size, hidden_sizes[0]),\n",
    "      torch.nn.ReLU(),\n",
    "      *[layer for i in range(len(hidden_sizes) - 1) for layer in [\n",
    "        torch.nn.Linear(hidden_sizes[i], hidden_sizes[i + 1]),\n",
    "        torch.nn.ReLU()\n",
    "      ]],\n",
    "      torch.nn.Linear(hidden_sizes[-1], output_size)\n",
    "    )\n",
    "\n",
    "  def forward(self, x): return self.model(x)\n",
    "\n",
    "  def sync_with(self, other: 'ModelModule'):\n",
    "    self.load_state_dict(other.state_dict())\n",
    "\n",
    "  def update_params(self, other: 'ModelModule', influence: float):\n",
    "    for owned, influencer in zip(self.parameters(), other.parameters()):\n",
    "      owned.data = owned.data * (1.0 - influence) + influencer.data * influence"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class NeuralQLearningAgent(Agent):\n",
    "  def __init__(\n",
    "      self, env: Env,\n",
    "      network: Configuration,\n",
    "      use_memory: Configuration = None,\n",
    "      use_double: Configuration = None,\n",
    "      use_freeze: Configuration = None\n",
    "  ):\n",
    "    super().__init__(env)\n",
    "\n",
    "    def create_model():\n",
    "      return ModelModule(self.env.observation_space.shape[0], network.layers, self.env.action_space.n)\n",
    "    self.env = env\n",
    "\n",
    "    self.uses_memory = bool(use_memory)\n",
    "    self.uses_double = bool(use_double)\n",
    "    self.uses_freeze = bool(use_freeze)\n",
    "\n",
    "    # Memory-parameters\n",
    "    self.memory = MemoryModule(use_memory.capacity)\n",
    "    self.update_frequency = use_memory.update_frequency\n",
    "    self.batch_size = use_memory.batch_size\n",
    "\n",
    "    # Double-parameters\n",
    "    self.online_model = create_model()\n",
    "    self.target_model = create_model()\n",
    "    self.target_model.sync_with(self.online_model)\n",
    "    self.influence = use_double.influence\n",
    "\n",
    "    # Freeze-parameters\n",
    "    self.freeze_frequency = use_freeze and use_freeze.frequency\n",
    "\n",
    "    # Hyper-parameters\n",
    "    self.optimizer = torch.optim.RMSprop(self.online_model.parameters(), lr=0.001)\n",
    "    self.loss_fn = torch.nn.MSELoss()\n",
    "    self.gamma = 0.99\n",
    "    self.epsilon = 1\n",
    "    self.epsilon_decay = 0.99\n",
    "    self.iterations = 0\n",
    "\n",
    "  def update(self, states, rewards, dones, online, target):\n",
    "    _, actions = online(states).max(dim=1, keepdim=True)\n",
    "    next_q_values = target(states).gather(dim=1, index=actions)\n",
    "    return rewards + self.gamma * next_q_values * (1 - dones)\n",
    "\n",
    "  def single_update(self, states, rewards, dones):\n",
    "    return self.update(states, rewards, dones, self.target_model, self.target_model)\n",
    "\n",
    "  def double_update(self, states, rewards, dones):\n",
    "    return self.update(states, rewards, dones, self.online_model, self.target_model)\n",
    "\n",
    "  def get_action(self, observation, learning):\n",
    "    if not self.can_train or (learning and np.random.random() < self.epsilon): return self.env.action_space.sample()\n",
    "\n",
    "    state = (torch.from_numpy(observation).unsqueeze(dim=0))\n",
    "    return self.online_model(state).argmax().item()\n",
    "\n",
    "  def train(self, experiences: List[Experience]) -> None:\n",
    "    states, actions, rewards, next_states, dones = map(Tensor, zip(*experiences))\n",
    "\n",
    "    actions = actions.long().unsqueeze(dim=1)\n",
    "    rewards = rewards.unsqueeze(dim=1)\n",
    "    dones = dones.unsqueeze(dim=1)\n",
    "\n",
    "    if self.uses_double:\n",
    "      target = self.double_update(next_states, rewards, dones)\n",
    "    else:\n",
    "      target = self.single_update(next_states, rewards, dones)\n",
    "\n",
    "    predicted = self.online_model(states).gather(dim=1, index=actions)\n",
    "    loss = self.loss_fn(predicted, target)\n",
    "\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "\n",
    "    self.target_model.update_params(self.online_model, self.influence)\n",
    "\n",
    "  @property\n",
    "  def should_train(self) -> bool:\n",
    "    return not self.uses_memory or self.iterations % self.update_frequency == 0\n",
    "\n",
    "  @property\n",
    "  def can_train(self) -> bool:\n",
    "    return not self.uses_memory or len(self.memory) >= self.batch_size\n",
    "\n",
    "  @property\n",
    "  def should_freeze(self):\n",
    "    return self.uses_freeze and self.iterations % self.freeze_frequency == 0\n",
    "\n",
    "  def process_transition(self, observation, action, reward, next_observation, done):\n",
    "    experience = Experience(observation, action, reward, next_observation, done)\n",
    "    self.memory.append(experience)\n",
    "    self.iterations += 1\n",
    "\n",
    "    if self.should_train and self.can_train:\n",
    "      if self.uses_memory:\n",
    "        self.train(self.memory.sample(self.batch_size))\n",
    "      else:\n",
    "        self.train([experience])\n",
    "\n",
    "      if not self.uses_freeze:\n",
    "        self.target_model.update_params(self.online_model, self.influence)\n",
    "\n",
    "    if self.should_freeze:\n",
    "      self.target_model.sync_with(self.online_model)\n",
    "\n",
    "    if done: self.update_epsilon()\n",
    "\n",
    "  def update_epsilon(self) -> float:\n",
    "    self.epsilon = max(0.01, self.epsilon * self.epsilon_decay)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check gpu\n",
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# check if used\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "network = Configuration(layers=[128, 128])\n",
    "memory = Configuration(update_frequency=4, batch_size=64, capacity=1_000_000)\n",
    "double = Configuration(influence=0.001)\n",
    "freeze = Configuration(frequency=25)\n",
    "agent = NeuralQLearningAgent(env, network=network, use_memory=memory, use_double=double, use_freeze=freeze)\n",
    "rewards = run_agent_in_env(env, agent, 10_000, learning=True, plot=True, plot_interval=100)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9cf6ace87ce140ee73c3212e83cfef8944681e1010745419fbed842180d29b9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
