{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from typing import List\n",
    "from random import choices\n",
    "from gymnasium import Env\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "  def __init__(self, env): ...\n",
    "  def process_transition(self, observation, action, reward, next_observation, done): ...\n",
    "  def get_action(self, observation, learning): ..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "outputs": [],
   "source": [
    "def select_greedy_actions(states: Tensor, model: nn.Module) -> Tensor:\n",
    "  \"\"\"Select the greedy action for the current state given some Q-network.\"\"\"\n",
    "  _, actions = model(states).max(dim=1, keepdim=True)\n",
    "  return actions\n",
    "\n",
    "def evaluate_actions(states: Tensor, actions: Tensor, rewards: Tensor, dones: Tensor, gamma: float, model: nn.Module) -> Tensor:\n",
    "  \"\"\"Compute the Q-values by evaluating the actions given the current states and Q-network.\"\"\"\n",
    "  next_q_values = model(states).gather(dim=1, index=actions)\n",
    "  q_values = rewards + (gamma * next_q_values * (1 - dones))\n",
    "  return q_values\n",
    "\n",
    "def single_update(states: Tensor, rewards: Tensor, dones: Tensor, gamma: float, network: nn.Module) -> Tensor:\n",
    "  \"\"\"Q-Learning update with explicitly decoupled action selection and evaluation steps.\"\"\"\n",
    "  actions = select_greedy_actions(states, network)\n",
    "  q_values = evaluate_actions(states, actions, rewards, dones, gamma, network)\n",
    "  return q_values\n",
    "\n",
    "def double_update(states: Tensor, rewards: Tensor, dones: Tensor, gamma: float, online: nn.Module,\n",
    "                  target: nn.Module) -> Tensor:\n",
    "  \"\"\"Double Q-Learning uses Q-network 1 to select actions and Q-network 2 to evaluate the selected actions.\"\"\"\n",
    "  actions = select_greedy_actions(states, online)\n",
    "  q_values = evaluate_actions(states, actions, rewards, dones, gamma, target)\n",
    "  return q_values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configuration(dict):\n",
    "  __getattr__ = dict.get\n",
    "  __setattr__ = dict.__setitem__\n",
    "  __delattr__ = dict.__delitem__\n",
    "\n",
    "Experience = namedtuple(\"Experience\", field_names=[\n",
    "  \"state\",\n",
    "  \"action\",\n",
    "  \"reward\",\n",
    "  \"next_state\",\n",
    "  \"done\"\n",
    "])\n",
    "class MemoryModule(object):\n",
    "  def __init__(self, capacity: int = None) -> None:\n",
    "    self.capacity = capacity\n",
    "    self.contents = []\n",
    "\n",
    "  def __len__(self) -> int:\n",
    "    return len(self.contents)\n",
    "\n",
    "  def append(self, experience: Experience) -> None:\n",
    "    if self.capacity and len(self) >= self.capacity: self.contents.pop(0)\n",
    "    self.contents.append(experience)\n",
    "\n",
    "  def sample(self, count: int) -> List[Experience]:\n",
    "    return choices(self.contents, k=count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "outputs": [],
   "source": [
    "class ModelModule(torch.nn.Module):\n",
    "  def __init__(self, input_size: int, hidden_sizes: List[int], output_size: int):\n",
    "    super().__init__()\n",
    "\n",
    "    self.model = torch.nn.Sequential(\n",
    "      torch.nn.Linear(input_size, hidden_sizes[0]),\n",
    "      torch.nn.ReLU(),\n",
    "      *[layer for i in range(len(hidden_sizes) - 1) for layer in [\n",
    "        torch.nn.Linear(hidden_sizes[i], hidden_sizes[i + 1]),\n",
    "        torch.nn.ReLU()\n",
    "      ]],\n",
    "      torch.nn.Linear(hidden_sizes[-1], output_size)\n",
    "    )\n",
    "\n",
    "  def forward(self, x): return self.model(x)\n",
    "\n",
    "  def sync_with(self, other: 'ModelModule'):\n",
    "    self.load_state_dict(other.state_dict())\n",
    "\n",
    "  def update_params(self, other: 'ModelModule', influence: float):\n",
    "    for owned, influencer in zip(self.parameters(), other.parameters()):\n",
    "      owned.data = owned.data * (1.0 - influence) + influencer.data * influence\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralQLearningAgent(Agent):\n",
    "  def __init__(\n",
    "      self, env: Env,\n",
    "      network: Configuration,\n",
    "      use_memory: Configuration = None,\n",
    "      use_double: Configuration = None,\n",
    "      use_freeze: Configuration = None\n",
    "  ):\n",
    "    super().__init__(env)\n",
    "    def create_model():\n",
    "      return ModelModule(self.env.observation_space.shape[0], network.layers, self.env.action_space.n)\n",
    "    self.env = env\n",
    "\n",
    "    self.uses_memory = bool(use_memory)\n",
    "    self.uses_double = bool(use_double)\n",
    "    self.uses_freeze = bool(use_freeze)\n",
    "\n",
    "    # Memory-parameters\n",
    "    self.memory = MemoryModule(use_memory.capacity)\n",
    "    self.update_frequency = use_memory.update_frequency\n",
    "    self.batch_size = use_memory.batch_size\n",
    "\n",
    "    # Double-parameters\n",
    "    self.online_model = create_model()\n",
    "    self.target_model = create_model()\n",
    "    self.target_model.sync_with(self.online_model)\n",
    "    self.influence = use_double.influence\n",
    "\n",
    "    # Freeze-parameters\n",
    "    self.freeze_frequency = use_freeze and use_freeze.frequency\n",
    "\n",
    "    # Hyper-parameters\n",
    "    self.optimizer = torch.optim.RMSprop(self.online_model.parameters(), lr=0.001)\n",
    "    self.gamma = 0.99\n",
    "    self.epsilon = 1\n",
    "    self.epsilon_decay = 0.99\n",
    "    self.iterations = 0\n",
    "\n",
    "  def get_action(self, observation, learning):\n",
    "    if not self.can_train or (learning and np.random.random() < self.epsilon): return self.env.action_space.sample()\n",
    "\n",
    "    state = (torch.from_numpy(observation).unsqueeze(dim=0))\n",
    "    return self.online_model(state).argmax().item()\n",
    "\n",
    "  def train(self, experiences: List[Experience]) -> None:\n",
    "    states, actions, rewards, next_states, dones = map(Tensor, zip(*experiences))\n",
    "\n",
    "    actions = actions.long().unsqueeze(dim=1)\n",
    "    rewards = rewards.unsqueeze(dim=1)\n",
    "    dones = dones.unsqueeze(dim=1)\n",
    "\n",
    "    if self.uses_double:\n",
    "      target = double_update(\n",
    "        next_states,\n",
    "        rewards,\n",
    "        dones,\n",
    "        self.gamma,\n",
    "        self.online_model,\n",
    "        self.target_model\n",
    "      )\n",
    "    else:\n",
    "      target = single_update(\n",
    "        next_states,\n",
    "        rewards,\n",
    "        dones,\n",
    "        self.gamma,\n",
    "        self.target_model\n",
    "      )\n",
    "\n",
    "    predicted = self.online_model(states).gather(dim=1, index=actions)\n",
    "    loss = F.mse_loss(predicted, target)\n",
    "\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "\n",
    "    self.target_model.update_params(self.online_model, self.influence)\n",
    "\n",
    "  @property\n",
    "  def should_train(self) -> bool:\n",
    "    return not self.uses_memory or self.iterations % self.update_frequency == 0\n",
    "\n",
    "  @property\n",
    "  def can_train(self) -> bool:\n",
    "    return not self.uses_memory or len(self.memory) >= self.batch_size\n",
    "\n",
    "  @property\n",
    "  def should_freeze(self):\n",
    "    return self.uses_freeze and self.iterations % self.freeze_frequency == 0\n",
    "\n",
    "  def process_transition(self, observation, action, reward, next_observation, done):\n",
    "    experience = Experience(observation, action, reward, next_observation, done)\n",
    "    self.memory.append(experience)\n",
    "    self.iterations += 1\n",
    "\n",
    "    if self.should_train and self.can_train:\n",
    "      if self.uses_memory:\n",
    "        self.train(self.memory.sample(self.batch_size))\n",
    "      else:\n",
    "        self.train([experience])\n",
    "\n",
    "      if not self.uses_freeze:\n",
    "        self.target_model.update_params(self.online_model, self.influence)\n",
    "\n",
    "    if self.should_freeze:\n",
    "      self.target_model.sync_with(self.online_model)\n",
    "\n",
    "    if done: self.update_epsilon()\n",
    "\n",
    "  def update_epsilon(self) -> float:\n",
    "    self.epsilon = max(0.01, self.epsilon * self.epsilon_decay)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for the training loop remains unchanged from the previous post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def train(agent: Agent, env: Env, episodes: int) -> List[float]:\n",
    "  def run_episode() -> float:\n",
    "    state, _ = env.reset()\n",
    "    score = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "      action = agent.get_action(state, True)\n",
    "      next_state, reward, done, term, _ = env.step(action)\n",
    "      agent.process_transition(state, action, reward, next_state, done or term)\n",
    "      state = next_state\n",
    "      score += reward\n",
    "    return score\n",
    "\n",
    "  scores = []\n",
    "  for episode in tqdm(range(episodes)):\n",
    "    scores.append(run_episode())\n",
    "\n",
    "    if (episode + 1) % 100 == 0:\n",
    "      average = np.mean(scores[-100:])\n",
    "      print(f\"\\rEpisode {episode + 1:<0}\\tAverage Score: {average:.2f}\")\n",
    "\n",
    "  return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing DQN and Double DQN\n",
    "\n",
    "To make it a bit easier to compare the overall performance of the two algorithms I will now re-train both agents for the same number of episodes (rather than training for the minimum number of episodes required to achieve a target score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "outputs": [],
   "source": [
    "network = Configuration(layers=[128, 128])\n",
    "memory = Configuration(update_frequency=4, batch_size=64, capacity=1_000_000)\n",
    "double = Configuration(influence=0.001)\n",
    "freeze = Configuration(frequency=25)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/2000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4eb34849f5da4d7aa43b1bc75e0092fa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 84.77\n",
      "Episode 200\tAverage Score: 299.44\n",
      "Episode 300\tAverage Score: 240.36\n",
      "Episode 400\tAverage Score: 212.32\n",
      "Episode 500\tAverage Score: 270.42\n",
      "Episode 600\tAverage Score: 326.61\n",
      "Episode 700\tAverage Score: 184.58\n",
      "Episode 800\tAverage Score: 238.25\n",
      "Episode 900\tAverage Score: 250.40\n",
      "Episode 1000\tAverage Score: 245.03\n",
      "Episode 1100\tAverage Score: 269.88\n",
      "Episode 1200\tAverage Score: 362.81\n",
      "Episode 1300\tAverage Score: 334.78\n",
      "Episode 1400\tAverage Score: 281.05\n",
      "Episode 1500\tAverage Score: 263.78\n",
      "Episode 1600\tAverage Score: 247.43\n",
      "Episode 1700\tAverage Score: 306.47\n",
      "Episode 1800\tAverage Score: 260.90\n",
      "Episode 1900\tAverage Score: 297.36\n",
      "Episode 2000\tAverage Score: 144.83\n"
     ]
    }
   ],
   "source": [
    "use_double_agent = NeuralQLearningAgent(env, network=network, use_double=double, use_memory=memory, use_freeze=freeze)\n",
    "\n",
    "use_double_scores = train(\n",
    "  use_double_agent,\n",
    "  env,\n",
    "  episodes=2000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_single_agent = NeuralQLearningAgent(env, network=network, use_double=False, use_memory=memory, use_freeze=freeze)\n",
    "\n",
    "use_single_scores = train(\n",
    "  use_single_agent,\n",
    "  env,\n",
    "  episodes=2000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_single_scores = pd.Series(use_single_scores, name=\"scores\")\n",
    "use_double_scores = pd.Series(use_double_scores, name=\"scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(10, 6), sharex=True, sharey=True)\n",
    "use_single_scores.plot(ax=axes[0], label=\"Single DQN Scores\")\n",
    "use_single_scores.rolling(window=100).mean().rename(\"Rolling Average\").plot(ax=axes[0])\n",
    "axes[0].legend()\n",
    "axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "use_double_scores.plot(ax=axes[1], label=\"Double DQN Scores\")\n",
    "use_double_scores.rolling(window=100).mean().rename(\"Rolling Average\").plot(ax=axes[1])\n",
    "axes[1].legend()\n",
    "axes[1].set_ylabel(\"Score\")\n",
    "axes[1].set_xlabel(\"Episode Number\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
