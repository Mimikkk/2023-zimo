{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zaawansowane Metody Inteligencji Obliczeniowej\n",
    "\n",
    "## Ćwiczenie: Stochastic Gradient Descent\n",
    "\n",
    "Celem tego ćwiczenia jest przyswojenie podstaw algorytmu Gradient Descent,\n",
    "regresji liniowej/logistycznej i sieci neuronowych.\n",
    "Ten notebook używa pewnych funkcji pomocniczych z pliku [zmio_sgd_helpers.py](zmio_sgd_helpers.py), lecz nie trzeba tam zaglądać.\n",
    "\n",
    "Ćwiczenie wymaga dodatkowych pakietów: **numpy**, **matplotlib**, **seaborn**, **pytorch**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linia potrzebna do rysowania wykresów\n",
    "# możesz ją zignorować, lecz odpal ten kod.\n",
    "%matplotlib inline\n",
    "\n",
    "# importy dodatkowych funkcji i pakietów\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "np.set_printoptions(precision = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stwórz dane dla regresji liniowej za pomocą dpstarczonej funkcji.\n",
    "x jest zbiorem obiektów z cechami (po jednej cesze dla obiektu w tym prostym przypadku). y Zawiera wartości, które chcemy przewidzieć na podstawie y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from zmio_sgd_helpers import generate_1d_regression_data\n",
    "\n",
    "x, y = generate_1d_regression_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zobacz jak wyglądają dane\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 1\n",
    "Twoim pierwszym zadaniem zaimplementowanie regresji liniowej (1-wymiarowej) dla średniego błędu kwadratowego:\n",
    "\n",
    "$$ argmin_{a,b \\in R } \\frac{1}{2n}\\sum\\limits_{i=0}^{n-1}(ax + b - y)^2$$\n",
    "\n",
    "Zauważ, że wektory wejściowe są 1-wymiarowe (są skalarami), ale problem optymalizacji jest 2-wymiarowy - optymalizujemy względem\n",
    "współczynnika nachylenia prostej **a** i wyrazu wolnego **b** (ang. bias term/intercept term)\n",
    "\n",
    "Zaimplementuj:\n",
    "* Rozwiązanie analityczne\n",
    "* Gradient Descent\n",
    "* Online Stochastic Gradient Descent\n",
    "* Mini-batch Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twoja implementacja powinna zwracać listę par (a,b), które zostały napotkane przez algorytm w czasie kolejnych iteracji.\n",
    "Będzie to przydatne dla gotowych już funkcji wizualizacji:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from zmio_sgd_helpers import plot_1d_regression_path, plot_1d_regression_lines\n",
    "\n",
    "\n",
    "#przykładowe pary rozwiązań:\n",
    "sample_points = [(0,0),(-3,1),(5,1),(6,4),(7,5)]\n",
    "\n",
    "#eysowanie ścieżki po jakiej poruszał się algorytm w przestrzeni parametrów i błąd w czasie\n",
    "plot_1d_regression_path(x, y, sample_points)\n",
    "\n",
    "#uzyskane proste regresji\n",
    "plot_1d_regression_lines(x, y, sample_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przetestuj różne: liczby iteracji, wielkości mini-batchy, punkty początkowe i prędkości uczenia. Sugerowane parametry do przetestowania:\n",
    " początkowe wagi: $(0,0)$, $\\alpha_0 = 0.1$, 5 iteracji, rozmiary mini-batchy $\n",
    "\\{5,10,20\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def exact_solution_1d(x, y):\n",
    "    # implementacja tutaj\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def online_sgd_1d(x, y, epochs, w0, b0,  alpha):\n",
    "    # implementacja tutaj\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def mini_batch_sgd_1d(x, y, epochs, w0, b0, batchsize, alpha):\n",
    "    # implementacja tutaj\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def gd_1d(x, y, epochs, w0, b0, alpha):\n",
    "    # implementacja tutaj\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 2\n",
    "Tym razem rozwiąż regresją liniową dla problemu o większym wymiarze.\n",
    "Nie używaj pętli tam gdzie nie jest to konieczne - użyj operacji macierzowych i wbudowanych funkcji macierzowych (sum, mean itd.).\n",
    "Uwaga: pętle do iteracji po danych będą konieczne.\n",
    "\n",
    "Funkcja generująca dane, robi to wedle deterministycznego modelu liniowego, którego parametry są zwracane razem z danymi\n",
    "(a_model i b_model) zatem możesz porównać wyniki (powoduje to też, że minimalny błąd to 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from zmio_sgd_helpers import generate_regression_data\n",
    "x, y, a_model, b_model = generate_regression_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rozwiąż regresję w sposób dokładny analitycznie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def mse_error(x, y, W, b):\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def exact_solution(x,y):\n",
    "    W, b = None, None\n",
    "    #implementacja\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "w_exact, b_exact = exact_solution(x,y)\n",
    "print(\"Błąd rozwiązania dokładnego (powinien wynieść ~0): {:0.5f}\".format( mse_error(x,y,w_exact,b_exact)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Twoja implementacja:\n",
    "Możesz zacząć od 100 iteracji, zerowego rozwiązania początkowego, wielkości mini-batchy 20 i prędkości uczenia $\\eta= 0.01$\n",
    "Spróbuj pozmieniać parametry (i dodać spadającą prędkość uczenia) by uzyskać jak najlepszy wynik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sgd(x, y):\n",
    "    W, b = None, None\n",
    "    errors = []\n",
    "    #implementacja\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "W, b, errors = sgd(x, y)\n",
    "plt.plot(errors)\n",
    "print(\"Błąd ostateczny sgd: {:0.5f}\".format(mse_error(x, y, W, b)))\n",
    "print(\"Rozwiązanie sgd: \", W, b)\n",
    "print(\"Rozwiązanie dokładne: \", W, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mnist\n",
    "Jednym z najbardziej typowych zbiorów danych by zapoznać się z problemem klasyfikacji jest MNIST.\n",
    "jest to zbiór obrazków ręcznie narysowanych cyfr. Obrazki są w odcieniach szarości i mają rozmiary 28x28 pikseli.\n",
    "Poniższy kod ściągnie (chyba, że dane już zostały pobrane wcześniej) zbiór do katalogu **mnist data** i wyświetli przykładową  cyfrę."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "mnist = datasets.MNIST(\"mnist_data\", download=True)\n",
    "images = np.array(mnist.data)\n",
    "labels = np.array(mnist.targets)\n",
    "\n",
    "random_integer = np.random.randint(0, len(mnist))\n",
    "image, label = images[random_integer], labels[random_integer]\n",
    "print(\"Powinniście zobaczyć cyfrę: {}\".format(label))\n",
    "plt.imshow(image)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Zadanie 3\n",
    "Zaimplementuj regresję logistyczną na zbiorze Mnist dla klasyfikacji binarnej (wybierz dwie klasy np. 0 i 1).\n",
    "\n",
    "Każde zdjęć to obrazek, a wejściem dla regresji logistycznej jest zbiór wektorów (macierz), musimy zatem spłaszczyć obrazki do postaci wektorów długości 28x28=784.\n",
    "\n",
    "Metoda spadku wzdłuż gradientu minimalizuje **funkcję straty** (ang. loss), lecz docelowo intereuje nas coś innego np. trafność predykcji (ang. accuracy),\n",
    "czyli jak często algorytm przewidział dobrą klasę (czy wiesz czemu nie używamy trafności do uczenia?).\n",
    "Poza błędem licz też średnią trafność predykcji w każdej iteracji.\n",
    "\n",
    "Przydatne funkcje:\n",
    "* funkcja logistyczna: $\\sigma(x) = \\frac{1}{1+e^{-x}}$\n",
    "* błąd logistyczny dla klas otagowanych jako 0 i 1:\n",
    "$\\mathcal{L}(W,b,x,y) = -\\frac{1}{n}\\sum\\limits_{i=0}^{n-1}y_i\\log(\\sigma(x_i^TW+b)) +(1-y_i)(\\log(1-\\sigma(x_i^TW+b)))$\n",
    "\n",
    "Uwaga: we wzorze pojawia się logarytm, a $\\log(0) \\rightarrow -\\infty$ co za tym idzie należy zadbać o to by nie spowodowało to kłopotów. Podobny numeryczny kłopot możemy napotkaś w funkcji wykładniczej obecnej w sigmoidzie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "small_constant = 1e-10\n",
    "\n",
    "def sigmoid(x):\n",
    "    raise NotImplementedError()\n",
    "\n",
    "# prediction(s) of your model before sigmoid\n",
    "def h_linear(x, params):\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def grad_linear(x, y, params):\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def logloss(logits, labels):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dataset(images, labels, classes=(0,1)):\n",
    "    allowed = np.zeros_like(labels, dtype=np.bool)\n",
    "        for ci, c in enumerate(classes):\n",
    "            allowed[labels == c] = True\n",
    "            labels[labels == c] = ci\n",
    "        x = images[allowed].astype(np.float64)\n",
    "\n",
    "        y = labels[allowed]\n",
    "        x = x.reshape((len(x), -1))\n",
    "        return x, y\n",
    "\n",
    "x, y = transform_dataset(images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def optimize(h, grad, params, epochs=10, alpha=0.0001, batchsize=512):\n",
    "    accuracies = []\n",
    "    losses = []\n",
    "    for e in range(epochs):\n",
    "        perm = np.random.permutation(len(x))\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    return accuracies, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize(h_linear, grad_linear, params, epochs=10, alpha=0.0001, batchsize=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdź czy twój model faktycznie robi uyteczne predykcje:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for flat_image, digit in zip(x,y):\n",
    "    # Twoja predykcja\n",
    "    plt.imshow(flat_image.reshape((28,28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 4\n",
    "\n",
    "Rozszerz rozwiązanie tak nasz model nie był liniowy, lecz by był 2 warstwową siecią neuronową z aktywacją w postaci sigmoida. \"Jedyne\" co musisz zmienić to model h i propagację gradientu. Pamiętaj by ostatnia warstwa sieci (ta, która następnie kompresowana jest przez sigmoid do prawdopodobieństwa) była zwykłym liniowym przekształceniem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def h_neural_network(x):\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def gradient_neural_network(x):\n",
    "    raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
