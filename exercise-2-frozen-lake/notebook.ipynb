{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zaawansowane Metody Inteligencji Obliczeniowej\n",
    "# Zadanie domowe 2\n",
    "### Prowadzący: Michał Kempka, Marek Wydmuch\n",
    "### Autor: Daniel Zdancewicz 145317"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wprowadzenie\n",
    "\n",
    "Całe zadanie jest oparte o różne wersje środowiska `FrozenLake` ze znanej biblioteki OpenAI Gym (https://gym.openai.com), która agreguje różnego rodzaju środowiska pod postacią jednego zunifikowanego API.\n",
    "\n",
    "Zapoznaj się z opisem środowiska (https://gym.openai.com/envs/FrozenLake-v0), a następnie zapoznaj się z kodem poniżej. Pokazuje on podstawy użytkowania API biblioteki Gym.\n",
    "\n",
    "#### Uwaga: Możesz dowolnie modyfikować elementy tego notebooka (wstawiać komórki i zmieniać kod) o ile nie napisano gdzieś inaczej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Zainstaluj bibliotekę OpenAI Gym w wersji 0.18.0\n",
    "!pip install gym == 0.18.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "import gym"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zad. 1 - Policy iteration + value iteration (10 pkt.)\n",
    "\n",
    "W komórkach poniżej zaimplementuj algorytmy **iteracji polityki** oraz **iteracji wartości**, wyznaczające deterministyczną politykę dla środowiska FrozenLake.\n",
    "\n",
    "Odpowiedź na pytania wykonując odpowiednie eksperymenty (zostaw output odpowiednich komórek na poparcie swoich twierdzeń):\n",
    "- Jak zmiana współczynniku `gamma` wpływa na wynikową politykę?\n",
    "- Jak stochastyczność wpływa na liczbę iteracji potrzebnych do zbiegnięcia obu algorytmów oraz wynikową politykę?\n",
    "\n",
    "#### Uwaga: nie zmieniaj nazwy funkcji `policy_iteration` i `value_iteration`, ani ich argumentów. Nie dopisuj do komórek z funkcjami innego kodu. Może zdefiniować funkcje pomocnicze dla danej funkcji w tej samej komórce (sprawdzarka wyciągnie ze zgłoszonego notebooka wyłącznie komórki zawierającą funkcje `policy_iteration` i `value_iteration` do sprawdzenia, kod w innych komórkach nie będzie widziany przez sprawdzarkę!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Odpowiedzi:\n",
    "1. Współczynnik `gamma` ustawiony w przedziale 0,0-0,1 lub o wartości 1.0 powoduje błędne polityki o złych liczbach iteracji, które powodują przegraną ciągłą przegraną.\n",
    "2. Stochastyczność wpływa na:\n",
    "- Liczba iteracji potrzebnych do zbiegnięcia:\n",
    "  - Stochastyczne — Potrzebujemy więcej iteracji przy większym gamma do zbiegnięcia. Może to wynikać ze względu na otrzebę uwzględnienia prawdopodobieństwa przejść pomiędzy stanami, co każe agentowi myśleć o ryzku związanymi z akcjami.\n",
    "  - Deterministyczne — zbiegają szybciej ze względu na brak potrzeby przewidywania przez agenta ryzyka swoich akcji.\n",
    "- Wynikowa polityka:\n",
    "  - Stochastyczne — Zdaje się ostrożna, agent możliwie uwzględnia ryzyko związane ze ślizganiem się na lodzie. Przez co możliwie, żę będzie rozważać ścieżki, które prowadzą do celu z mniejszym ryzykiem, ale są dłuższe.\n",
    "  - Deterministyczne — Agent może zawsze wybrać najkrótszą ścieżkę do celu ze względu na pewność wykonanych akcji przez agenta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "def evaluate_empirically(env, pi, episodes=1000, max_actions=100):\n",
    "  mean_reward = 0\n",
    "  for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    for _ in range(max_actions):\n",
    "      state, reward, termination, _ = env.step(pi[state])\n",
    "      total_reward += reward\n",
    "      if termination: break\n",
    "    mean_reward = mean_reward + 1 / (episode + 1) * (total_reward - mean_reward)\n",
    "  return mean_reward"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(P, gamma, delta=0.001):\n",
    "  \"\"\"\n",
    "  Argumenty:\n",
    "      P — model przejścia, gdzie P[s][a] == [(probability, nextstate, reward, done), ...]\n",
    "      gamma — współczynnik dyskontujący\n",
    "      delta — tolerancja warunku stopu\n",
    "  Zwracane wartości:\n",
    "      V — lista o długości len(P) zawierający oszacowane wartość stanu s: V[s]\n",
    "      pi — lista o długości len(P) zawierający wyznaczoną deterministyczną politykę - akcję dla stanu s: pi[s]\n",
    "      i — ilość iteracji algorytmu po wszystkich stanach\n",
    "  \"\"\"\n",
    "  def evaluate():\n",
    "    while True:\n",
    "      score = 0\n",
    "      for state in states:\n",
    "        value = V[state]\n",
    "        action = policy[state]\n",
    "        V[state] = sum(\n",
    "          probability * (reward + gamma * V[next]) for probability, next, reward, _ in P[state][action]\n",
    "        )\n",
    "        score = max(score, abs(value - V[state]))\n",
    "      if score < delta: break\n",
    "\n",
    "  def improve():\n",
    "    is_stable = True\n",
    "    for state in states:\n",
    "      previous = policy[state]\n",
    "      actions = range(len(P[state]))\n",
    "      policy[state] = max(actions, key=lambda action: sum(\n",
    "        probability * (reward + gamma * V[next]) for probability, next, reward, _ in P[state][action]\n",
    "      ))\n",
    "\n",
    "      if previous != policy[state]: is_stable = False\n",
    "    return is_stable\n",
    "\n",
    "  V = [0] * len(P)\n",
    "  policy = [0] * len(P)\n",
    "  states = range(len(P))\n",
    "  iterations = 0\n",
    "\n",
    "  while True:\n",
    "    iterations += 1\n",
    "    evaluate()\n",
    "    is_stable = improve()\n",
    "    if is_stable: break\n",
    "\n",
    "  return V, policy, iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wartości stanów (V):\n",
      "[0.03125, 0.0625, 0.125, 0.0625, 0.0625, 0.0, 0.25, 0.0, 0.125, 0.25, 0.5, 0.0, 0.0, 0.5, 1.0, 0.0]\n",
      "Polityka (pi):\n",
      "[1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0]\n",
      "Ilość iteracji: 7\n"
     ]
    },
    {
     "data": {
      "text/plain": "1.0"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "P = env.P\n",
    "gamma = 0.5\n",
    "V, policy, iterations = policy_iteration(P, gamma)\n",
    "\n",
    "print(f\"Wartości stanów (V):\\n{V}\")\n",
    "print(f\"Polityka (pi):\\n{policy}\")\n",
    "print(f\"Ilość iteracji: {iterations}\")\n",
    "\n",
    "evaluate_empirically(env, policy)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "def value_iteration(P, gamma, delta=0.001):\n",
    "  \"\"\"\n",
    "  Argumenty:\n",
    "      P — model przejścia, gdzie P[s][a] == [(probability, nextstate, reward, done), ...]\n",
    "      gamma — współczynnik dyskontujący\n",
    "      delta — tolerancja warunku stopu\n",
    "  Zwracane wartości:\n",
    "      Q — lista o długości len(P) zawierający listy z oszacowanymi wartościami dla stanu s i akcji a: Q[s][a]\n",
    "      pi — lista o długości len(P) zawierający wyznaczoną deterministyczną politykę - akcję dla stanu s: pi[s]\n",
    "      i — ilość iteracji algorytmu po wszystkich stanach\n",
    "  \"\"\"\n",
    "  def evaluate():\n",
    "    score = 0\n",
    "    for state in states:\n",
    "      for action in range(len(P[state])):\n",
    "        q = Q[state][action]\n",
    "        Q[state][action] = sum([p * (r + gamma * max(Q[s_])) for p, s_, r, _ in P[state][action]])\n",
    "        score = max(score, abs(q - Q[state][action]))\n",
    "    return score\n",
    "\n",
    "  def extract():\n",
    "    return [Q[state].index(max(Q[state])) for state in states]\n",
    "\n",
    "  states = P.keys()\n",
    "  Q = [[0] * len(P[state]) for state in states]\n",
    "  iterations = 0\n",
    "\n",
    "  while True:\n",
    "    iterations += 1\n",
    "    score = evaluate()\n",
    "    if score < delta: break\n",
    "\n",
    "  pi = extract()\n",
    "  return Q, pi, iterations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wartości stanów i akcji (Q):\n",
      "[,\n",
      "[0.015625, 0.03125, 0.03125, 0.015625],\n",
      "[0.015625, 0.0, 0.0625, 0.03125],\n",
      "[0.03125, 0.125, 0.03125, 0.0625],\n",
      "[0.0625, 0.0, 0.03125, 0.03125],\n",
      "[0.03125, 0.0625, 0.0, 0.015625],\n",
      "[0.0, 0.0, 0.0, 0.0],\n",
      "[0.0, 0.25, 0.0, 0.0625],\n",
      "[0.0, 0.0, 0.0, 0.0],\n",
      "[0.0625, 0.0, 0.125, 0.03125],\n",
      "[0.0625, 0.25, 0.25, 0.0],\n",
      "[0.125, 0.5, 0.0, 0.125],\n",
      "[0.0, 0.0, 0.0, 0.0],\n",
      "[0.0, 0.0, 0.0, 0.0],\n",
      "[0.0, 0.25, 0.5, 0.125],\n",
      "[0.25, 0.5, 1.0, 0.25],\n",
      "[0.0, 0.0, 0.0, 0.0],\n",
      "]\n",
      "Polityka (pi):\n",
      "[1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0]\n",
      "Ilość iteracji: 8\n"
     ]
    },
    {
     "data": {
      "text/plain": "1.0"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "P = env.P\n",
    "gamma = 0.5\n",
    "Q, policy, iterations = value_iteration(P, gamma)\n",
    "\n",
    "print(f\"Wartości stanów i akcji (Q):\")\n",
    "print('[', *Q, ']', sep=',\\n')\n",
    "print(f\"Polityka (pi):\\n{policy}\")\n",
    "print(f\"Ilość iteracji: {iterations}\")\n",
    "evaluate_empirically(env, policy)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration:\n",
      "Gamma: 0.0, Polityka: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0], Liczba iteracji: 2\n",
      "Wynik: 0.0\n",
      "Gamma: 0.1, Polityka: [1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0], Liczba iteracji: 7\n",
      "Wynik: 1.0\n",
      "Gamma: 0.5, Polityka: [1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0], Liczba iteracji: 7\n",
      "Wynik: 1.0\n",
      "Gamma: 0.9, Polityka: [1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0], Liczba iteracji: 7\n",
      "Wynik: 1.0\n",
      "Gamma: 0.99, Polityka: [1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0], Liczba iteracji: 7\n",
      "Wynik: 1.0\n",
      "Gamma: 1, Polityka: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0], Liczba iteracji: 8\n",
      "Wynik: 0.0\n",
      "\n",
      "Value Iteration:\n",
      "Gamma: 0.0, Polityka: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0], Liczba iteracji: 2\n",
      "Wynik: 0.0\n",
      "Gamma: 0.1, Polityka: [0, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0], Liczba iteracji: 5\n",
      "Wynik: 0.0\n",
      "Gamma: 0.5, Polityka: [1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0], Liczba iteracji: 8\n",
      "Wynik: 1.0\n",
      "Gamma: 0.9, Polityka: [1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0], Liczba iteracji: 8\n",
      "Wynik: 1.0\n",
      "Gamma: 0.99, Polityka: [1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0], Liczba iteracji: 8\n",
      "Wynik: 1.0\n",
      "Gamma: 1, Polityka: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0], Liczba iteracji: 8\n",
      "Wynik: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Eksperyment wpływu `gamma`.\n",
    "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "P = env.P\n",
    "gammas = [0.0, 0.1, 0.5, 0.9, 0.99, 1]\n",
    "\n",
    "print(\"Policy Iteration:\")\n",
    "for gamma in gammas:\n",
    "  V, pi, i = policy_iteration(P, gamma)\n",
    "  print(f\"Gamma: {gamma}, Polityka: {pi}, Liczba iteracji: {i}\")\n",
    "  print(f\"Wynik: {evaluate_empirically(env, pi)}\")\n",
    "\n",
    "print(\"\\nValue Iteration:\")\n",
    "for gamma in gammas:\n",
    "  Q, pi, i = value_iteration(P, gamma)\n",
    "  print(f\"Gamma: {gamma}, Polityka: {pi}, Liczba iteracji: {i}\")\n",
    "  print(f\"Wynik: {evaluate_empirically(env, pi)}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration:\n",
      "Deterministyczny: Polityka: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0], Liczba iteracji: 2\n",
      "Wynik: 0.0\n",
      "Stochastyczny: Polityka: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], Liczba iteracji: 2\n",
      "Wynik: 0.0\n",
      "Deterministyczny: Polityka: [1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0], Liczba iteracji: 7\n",
      "Wynik: 1.0\n",
      "Stochastyczny: Polityka: [1, 3, 2, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0], Liczba iteracji: 6\n",
      "Wynik: 0.474\n",
      "Deterministyczny: Polityka: [1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0], Liczba iteracji: 7\n",
      "Wynik: 1.0\n",
      "Stochastyczny: Polityka: [1, 3, 2, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0], Liczba iteracji: 5\n",
      "Wynik: 0.41300000000000003\n",
      "Deterministyczny: Polityka: [1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0], Liczba iteracji: 7\n",
      "Wynik: 1.0\n",
      "Stochastyczny: Polityka: [0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0], Liczba iteracji: 6\n",
      "Wynik: 0.727000000000001\n",
      "Deterministyczny: Polityka: [1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0], Liczba iteracji: 7\n",
      "Wynik: 1.0\n",
      "Stochastyczny: Polityka: [0, 3, 3, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0], Liczba iteracji: 7\n",
      "Wynik: 0.7520000000000003\n",
      "Deterministyczny: Polityka: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0], Liczba iteracji: 8\n",
      "Wynik: 0.0\n",
      "Stochastyczny: Polityka: [0, 3, 3, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0], Liczba iteracji: 7\n",
      "Wynik: 0.7279999999999992\n",
      "\n",
      "Value Iteration:\n",
      "Deterministyczny: Polityka: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0], Liczba iteracji: 2\n",
      "Wynik: 0.0\n",
      "Stochastyczny: Polityka: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], Liczba iteracji: 2\n",
      "Wynik: 0.0\n",
      "Deterministyczny: Polityka: [0, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0], Liczba iteracji: 5\n",
      "Wynik: 0.0\n",
      "Stochastyczny: Polityka: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 1, 0], Liczba iteracji: 3\n",
      "Wynik: 0.0\n",
      "Deterministyczny: Polityka: [1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0], Liczba iteracji: 8\n",
      "Wynik: 1.0\n",
      "Stochastyczny: Polityka: [2, 3, 2, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0], Liczba iteracji: 6\n",
      "Wynik: 0.48299999999999976\n",
      "Deterministyczny: Polityka: [1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0], Liczba iteracji: 8\n",
      "Wynik: 1.0\n",
      "Stochastyczny: Polityka: [0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0], Liczba iteracji: 21\n",
      "Wynik: 0.7230000000000003\n",
      "Deterministyczny: Polityka: [1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0], Liczba iteracji: 8\n",
      "Wynik: 1.0\n",
      "Stochastyczny: Polityka: [0, 3, 3, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0], Liczba iteracji: 82\n",
      "Wynik: 0.7379999999999999\n",
      "Deterministyczny: Polityka: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0], Liczba iteracji: 8\n",
      "Wynik: 0.0\n",
      "Stochastyczny: Polityka: [0, 3, 3, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0], Liczba iteracji: 117\n",
      "Wynik: 0.7460000000000001\n"
     ]
    }
   ],
   "source": [
    "# Eksperyment środowiska deterministycznego kontra stochastycznego.\n",
    "env_deterministic = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "env_stochastic = gym.make('FrozenLake-v0', is_slippery=True)\n",
    "P_deterministic = env_deterministic.P\n",
    "P_stochastic = env_stochastic.P\n",
    "\n",
    "gammas = [0.0, 0.1, 0.5, 0.9, 0.99, 1]\n",
    "\n",
    "print(\"Policy Iteration:\")\n",
    "for gamma in gammas:\n",
    "  V_det, pi_det, i_det = policy_iteration(P_deterministic, gamma)\n",
    "  V_sto, pi_sto, i_sto = policy_iteration(P_stochastic, gamma)\n",
    "  print(f\"Deterministyczny: Polityka: {pi_det}, Liczba iteracji: {i_det}\")\n",
    "  print(f\"Wynik: {evaluate_empirically(env_deterministic, pi_det)}\")\n",
    "  print(f\"Stochastyczny: Polityka: {pi_sto}, Liczba iteracji: {i_sto}\")\n",
    "  print(f\"Wynik: {evaluate_empirically(env_stochastic, pi_sto)}\")\n",
    "\n",
    "print(\"\\nValue Iteration:\")\n",
    "for gamma in gammas:\n",
    "  Q_det, pi_det, i_det = value_iteration(P_deterministic, gamma)\n",
    "  Q_sto, pi_sto, i_sto = value_iteration(P_stochastic, gamma)\n",
    "  print(f\"Deterministyczny: Polityka: {pi_det}, Liczba iteracji: {i_det}\")\n",
    "  print(f\"Wynik: {evaluate_empirically(env_deterministic, pi_det)}\")\n",
    "  print(f\"Stochastyczny: Polityka: {pi_sto}, Liczba iteracji: {i_sto}\")\n",
    "  print(f\"Wynik: {evaluate_empirically(env_stochastic, pi_sto)}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zad. 2 - Monte Carlo (10 pkt.)\n",
    "W komórce poniżej zaimplementuj metodę **On-policy Monte Carlo** dla polityki epsilon-greedy.\n",
    "Zakładamy, że model przejść nie jest w tym wypadku dla nas dostępny,\n",
    "dlatego możesz używać wyłącznie metod `env.reset()` i `env.step()`\n",
    "w swojej implementacji, w celu wygenerowania nowego epizodu.\n",
    "\n",
    "- Zaproponuj warunek stopu dla swojej implementacji.\n",
    "- Jaki jest wpływ epsilony na działanie algorytmu?\n",
    "- Jaka prosta modyfikacja nagród środowiska przyśpieszyłaby odkrywanie dobrej polityki? Zmodyfikuj env.P i zademonstruj.\n",
    "\n",
    "Tip: z racji, że env.P jest dostępne, możesz porównać wyniki `on_policy_eps_greedy_monte_carlo` ze wynikami `value_iteration`. \n",
    "\n",
    "#### Uwaga: nie zmieniaj nazwy funkcji `on_policy_eps_greedy_monte_carlo`, ani jej pierwszych argumentów (możesz dodać nowe argumenty z wartościami domyślnymi). Nie dopisuj do komórki z funkcją innego kodu. Może zdefiniować funkcje pomocnicze dla funkcji w tej samej komórce (sprawdzarka wyciągnie ze zgłoszonego notebooka wyłącznie komórkę zawierającą funkcję `on_policy_eps_greedy_monte_carlo` do sprawdzenia, kod w innych komórkach nie będzie widziany przez sprawdzarkę!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Odpowiedź: Miejsce na Twoje odpowiedzi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "def modify_rewards(P, goal, reward_step=-0.01, reward_goal=1.0):\n",
    "  P = P.copy()\n",
    "  for state in range(len(P)):\n",
    "    for action in range(len(P[state])):\n",
    "      P[state][action] = [\n",
    "        (probability, next, next == goal and reward_goal or reward_step, done)\n",
    "        for (probability, next, reward, done) in P[state][action]\n",
    "      ]\n",
    "  return P\n",
    "\n",
    "def on_policy_eps_greedy_monte_carlo(env, eps, gamma, max_episodes=10000):\n",
    "  \"\"\"\n",
    "  Argumenty:\n",
    "      env — środowisko implementujące metody `reset()` oraz `step(action)`\n",
    "      eps — współczynnik eksploracji\n",
    "      gamma — współczynnik dyskontujący\n",
    "  Zwracane wartości:\n",
    "      Q — lista o długości len(P) zawierający listy z oszacowanymi wartościami dla stanu s i akcji a: Q[s][a]\n",
    "      pi — lista o długości len(P) zawierający wyznaczoną deterministyczną (zachłanną) politykę - akcję dla stanu s: pi[s]\n",
    "      i — ilość epizodów wygenerowanych przez algorytm\n",
    "  \"\"\"\n",
    "  def generate_episodes():\n",
    "    nonlocal iterations\n",
    "    while (iterations := iterations + 1) < max_episodes:\n",
    "      state = env.reset()\n",
    "      episode = []\n",
    "      termination = False\n",
    "\n",
    "      while not termination:\n",
    "        action = pi[state] if random.random() > eps else random.randrange(len(P[state]))\n",
    "        next_state, reward, termination, _ = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "\n",
    "      yield episode\n",
    "\n",
    "  def evaluate(episode):\n",
    "    result = 0\n",
    "\n",
    "    for (state, action, reward) in reversed(episode):\n",
    "      result = gamma * result + reward\n",
    "\n",
    "      counts[state][action] += 1\n",
    "      Q[state][action] += (result - Q[state][action]) / counts[state][action]\n",
    "\n",
    "  def extract():\n",
    "    return [Q[state].index(max(Q[state])) for state in range(len(Q))]\n",
    "\n",
    "  P = env.P\n",
    "  states = range(len(P))\n",
    "  pi = [0] * len(states)\n",
    "  Q = [[0] * len(P[s]) for s in states]\n",
    "  counts = [[0] * len(P[state]) for state in states]\n",
    "  iterations = 0\n",
    "\n",
    "  episode_it = generate_episodes()\n",
    "  while episode := next(episode_it, None):\n",
    "    evaluate(episode)\n",
    "    pi = extract()\n",
    "\n",
    "  return Q, pi, iterations\n",
    "\n",
    "def on_policy_eps_greedy_monte_carlo(env, eps, gamma, max_episodes=5000, improvement_threshold=0.001):\n",
    "  def eps_greedy_policy(Q, state, eps):\n",
    "    if random.random() < eps:\n",
    "      return random.randint(0, len(Q[state]) - 1)\n",
    "    return max(range(len(Q[state])), key=lambda a: Q[state][a])\n",
    "\n",
    "  pi = [0] * len(env.P)\n",
    "  Q = defaultdict(lambda: [0] * len(env.P[0]))\n",
    "  returns = defaultdict(list)\n",
    "  i = 0\n",
    "\n",
    "  while i < max_episodes:\n",
    "    i += 1\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "      action = eps_greedy_policy(Q, state, eps)\n",
    "      next_state, reward, done, _ = env.step(action)\n",
    "      episode.append((state, action, reward))\n",
    "      state = next_state\n",
    "\n",
    "    G = 0\n",
    "    visited = set()\n",
    "    for t in reversed(range(len(episode))):\n",
    "      state, action, reward = episode[t]\n",
    "      G = gamma * G + reward\n",
    "\n",
    "      if (state, action) not in visited:\n",
    "        visited.add((state, action))\n",
    "        returns[(state, action)].append(G)\n",
    "        Q[state][action] = sum(returns[(state, action)]) / len(returns[(state, action)])\n",
    "        old_action = pi[state]\n",
    "        pi[state] = max(range(len(Q[state])), key=lambda a: Q[state][a])\n",
    "\n",
    "        if abs(Q[state][old_action] - Q[state][pi[state]]) > improvement_threshold:\n",
    "          break\n",
    "  return Q, pi, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wartości stanów i akcji (P):\n",
      "(0, {0: [(0.3333333333333333, 0, -0.02, False), (0.3333333333333333, 0, -0.02, False), (0.3333333333333333, 4, -0.02, False)], 1: [(0.3333333333333333, 0, -0.02, False), (0.3333333333333333, 4, -0.02, False), (0.3333333333333333, 1, -0.02, False)], 2: [(0.3333333333333333, 4, -0.02, False), (0.3333333333333333, 1, -0.02, False), (0.3333333333333333, 0, -0.02, False)], 3: [(0.3333333333333333, 1, -0.02, False), (0.3333333333333333, 0, -0.02, False), (0.3333333333333333, 0, -0.02, False)]})\n",
      "(1, {0: [(0.3333333333333333, 1, -0.02, False), (0.3333333333333333, 0, -0.02, False), (0.3333333333333333, 5, -0.02, True)], 1: [(0.3333333333333333, 0, -0.02, False), (0.3333333333333333, 5, -0.02, True), (0.3333333333333333, 2, -0.02, False)], 2: [(0.3333333333333333, 5, -0.02, True), (0.3333333333333333, 2, -0.02, False), (0.3333333333333333, 1, -0.02, False)], 3: [(0.3333333333333333, 2, -0.02, False), (0.3333333333333333, 1, -0.02, False), (0.3333333333333333, 0, -0.02, False)]})\n",
      "(2, {0: [(0.3333333333333333, 2, -0.02, False), (0.3333333333333333, 1, -0.02, False), (0.3333333333333333, 6, -0.02, False)], 1: [(0.3333333333333333, 1, -0.02, False), (0.3333333333333333, 6, -0.02, False), (0.3333333333333333, 3, -0.02, False)], 2: [(0.3333333333333333, 6, -0.02, False), (0.3333333333333333, 3, -0.02, False), (0.3333333333333333, 2, -0.02, False)], 3: [(0.3333333333333333, 3, -0.02, False), (0.3333333333333333, 2, -0.02, False), (0.3333333333333333, 1, -0.02, False)]})\n",
      "(3, {0: [(0.3333333333333333, 3, -0.02, False), (0.3333333333333333, 2, -0.02, False), (0.3333333333333333, 7, -0.02, True)], 1: [(0.3333333333333333, 2, -0.02, False), (0.3333333333333333, 7, -0.02, True), (0.3333333333333333, 3, -0.02, False)], 2: [(0.3333333333333333, 7, -0.02, True), (0.3333333333333333, 3, -0.02, False), (0.3333333333333333, 3, -0.02, False)], 3: [(0.3333333333333333, 3, -0.02, False), (0.3333333333333333, 3, -0.02, False), (0.3333333333333333, 2, -0.02, False)]})\n",
      "(4, {0: [(0.3333333333333333, 0, -0.02, False), (0.3333333333333333, 4, -0.02, False), (0.3333333333333333, 8, -0.02, False)], 1: [(0.3333333333333333, 4, -0.02, False), (0.3333333333333333, 8, -0.02, False), (0.3333333333333333, 5, -0.02, True)], 2: [(0.3333333333333333, 8, -0.02, False), (0.3333333333333333, 5, -0.02, True), (0.3333333333333333, 0, -0.02, False)], 3: [(0.3333333333333333, 5, -0.02, True), (0.3333333333333333, 0, -0.02, False), (0.3333333333333333, 4, -0.02, False)]})\n",
      "(5, {0: [(1.0, 5, -0.02, True)], 1: [(1.0, 5, -0.02, True)], 2: [(1.0, 5, -0.02, True)], 3: [(1.0, 5, -0.02, True)]})\n",
      "(6, {0: [(0.3333333333333333, 2, -0.02, False), (0.3333333333333333, 5, -0.02, True), (0.3333333333333333, 10, -0.02, False)], 1: [(0.3333333333333333, 5, -0.02, True), (0.3333333333333333, 10, -0.02, False), (0.3333333333333333, 7, -0.02, True)], 2: [(0.3333333333333333, 10, -0.02, False), (0.3333333333333333, 7, -0.02, True), (0.3333333333333333, 2, -0.02, False)], 3: [(0.3333333333333333, 7, -0.02, True), (0.3333333333333333, 2, -0.02, False), (0.3333333333333333, 5, -0.02, True)]})\n",
      "(7, {0: [(1.0, 7, -0.02, True)], 1: [(1.0, 7, -0.02, True)], 2: [(1.0, 7, -0.02, True)], 3: [(1.0, 7, -0.02, True)]})\n",
      "(8, {0: [(0.3333333333333333, 4, -0.02, False), (0.3333333333333333, 8, -0.02, False), (0.3333333333333333, 12, -0.02, True)], 1: [(0.3333333333333333, 8, -0.02, False), (0.3333333333333333, 12, -0.02, True), (0.3333333333333333, 9, -0.02, False)], 2: [(0.3333333333333333, 12, -0.02, True), (0.3333333333333333, 9, -0.02, False), (0.3333333333333333, 4, -0.02, False)], 3: [(0.3333333333333333, 9, -0.02, False), (0.3333333333333333, 4, -0.02, False), (0.3333333333333333, 8, -0.02, False)]})\n",
      "(9, {0: [(0.3333333333333333, 5, -0.02, True), (0.3333333333333333, 8, -0.02, False), (0.3333333333333333, 13, -0.02, False)], 1: [(0.3333333333333333, 8, -0.02, False), (0.3333333333333333, 13, -0.02, False), (0.3333333333333333, 10, -0.02, False)], 2: [(0.3333333333333333, 13, -0.02, False), (0.3333333333333333, 10, -0.02, False), (0.3333333333333333, 5, -0.02, True)], 3: [(0.3333333333333333, 10, -0.02, False), (0.3333333333333333, 5, -0.02, True), (0.3333333333333333, 8, -0.02, False)]})\n",
      "(10, {0: [(0.3333333333333333, 6, -0.02, False), (0.3333333333333333, 9, -0.02, False), (0.3333333333333333, 14, -0.02, False)], 1: [(0.3333333333333333, 9, -0.02, False), (0.3333333333333333, 14, -0.02, False), (0.3333333333333333, 11, -0.02, True)], 2: [(0.3333333333333333, 14, -0.02, False), (0.3333333333333333, 11, -0.02, True), (0.3333333333333333, 6, -0.02, False)], 3: [(0.3333333333333333, 11, -0.02, True), (0.3333333333333333, 6, -0.02, False), (0.3333333333333333, 9, -0.02, False)]})\n",
      "(11, {0: [(1.0, 11, -0.02, True)], 1: [(1.0, 11, -0.02, True)], 2: [(1.0, 11, -0.02, True)], 3: [(1.0, 11, -0.02, True)]})\n",
      "(12, {0: [(1.0, 12, -0.02, True)], 1: [(1.0, 12, -0.02, True)], 2: [(1.0, 12, -0.02, True)], 3: [(1.0, 12, -0.02, True)]})\n",
      "(13, {0: [(0.3333333333333333, 9, -0.02, False), (0.3333333333333333, 12, -0.02, True), (0.3333333333333333, 13, -0.02, False)], 1: [(0.3333333333333333, 12, -0.02, True), (0.3333333333333333, 13, -0.02, False), (0.3333333333333333, 14, -0.02, False)], 2: [(0.3333333333333333, 13, -0.02, False), (0.3333333333333333, 14, -0.02, False), (0.3333333333333333, 9, -0.02, False)], 3: [(0.3333333333333333, 14, -0.02, False), (0.3333333333333333, 9, -0.02, False), (0.3333333333333333, 12, -0.02, True)]})\n",
      "(14, {0: [(0.3333333333333333, 10, -0.02, False), (0.3333333333333333, 13, -0.02, False), (0.3333333333333333, 14, -0.02, False)], 1: [(0.3333333333333333, 13, -0.02, False), (0.3333333333333333, 14, -0.02, False), (0.3333333333333333, 15, 1, True)], 2: [(0.3333333333333333, 14, -0.02, False), (0.3333333333333333, 15, 1, True), (0.3333333333333333, 10, -0.02, False)], 3: [(0.3333333333333333, 15, 1, True), (0.3333333333333333, 10, -0.02, False), (0.3333333333333333, 13, -0.02, False)]})\n",
      "(15, {0: [(1.0, 15, 1, True)], 1: [(1.0, 15, 1, True)], 2: [(1.0, 15, 1, True)], 3: [(1.0, 15, 1, True)]})\n",
      "Wartości stanów i akcji (Q):\n",
      "defaultdict(<function on_policy_eps_greedy_monte_carlo.<locals>.<lambda> at 0x7f59a00a7940>, {0: [-0.07936448130181128, -0.07555077696083681, -0.06674781084983555, -0.08386626928944464], 4: [-0.06630351319307158, -0.03701512529306874, -0.05118206778177519, -0.055997970070715414], 8: [-0.04465423573958848, -0.024675392759664152, -0.025972261690238276, -0.03362782566721069], 9: [0.017104905631275386, -0.002417252521226475, 0.033260105378451026, -0.019572015218501614], 13: [0.03350496992675474, 0.14210766648451356, 0.15620750810508927, 0.06663655285128373], 10: [0.06876231557840756, 0.09353623628943988, 0.11096295630042256, -0.019202629145821076], 1: [-0.06003212109332092, -0.057267337630828266, -0.04751036260418422, -0.07894132845689839], 2: [-0.06003745545972263, -0.051815200174561614, -0.06028798115693796, -0.07304445460154761], 6: [-0.0099043323034139, -0.0033566293957339507, -0.03322367018399772, -0.0400272776658797], 14: [0.1271379041981558, 0.5005676243899795, 0.5630370827101219, 0.37727640436075766], 3: [-0.055125605298120085, -0.04002097401592497, -0.05202305224908796, -0.0717592080400477]})\n",
      "Polityka (pi):\n",
      "[2, 2, 1, 1, 1, 0, 1, 0, 1, 2, 2, 0, 0, 2, 2, 0]\n",
      "Ilość epizodów: 10000\n"
     ]
    },
    {
     "data": {
      "text/plain": "-0.06596"
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0', is_slippery=True)\n",
    "epsilon = 0.7\n",
    "gamma = 0.9\n",
    "max_episodes = 10_000\n",
    "env.P = modify_rewards(env.P, goal=15, reward_goal=1, reward_step=-0.02)\n",
    "Q, pi, i = on_policy_eps_greedy_monte_carlo(env, epsilon, gamma, max_episodes)\n",
    "print(f\"Wartości stanów i akcji (P):\")\n",
    "print(*env.P.items(), sep='\\n')\n",
    "print(f\"Wartości stanów i akcji (Q):\\n{Q}\")\n",
    "print(f\"Polityka (pi):\\n{pi}\")\n",
    "print(f\"Ilość epizodów: {i}\")\n",
    "evaluate_empirically(env, pi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
