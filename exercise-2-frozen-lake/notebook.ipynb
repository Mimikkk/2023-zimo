{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zaawansowane Metody Inteligencji Obliczeniowej\n",
    "# Zadanie domowe 2\n",
    "### Prowadzący: Michał Kempka, Marek Wydmuch\n",
    "### Autor: Daniel Zdancewicz 145317"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wprowadzenie\n",
    "\n",
    "Całe zadanie jest oparte o różne wersje środowiska `FrozenLake` ze znanej biblioteki OpenAI Gym (https://gym.openai.com), która agreguje różnego rodzaju środowiska pod postacią jednego zunifikowanego API.\n",
    "\n",
    "Zapoznaj się z opisem środowiska (https://gym.openai.com/envs/FrozenLake-v0), a następnie zapoznaj się z kodem poniżej. Pokazuje on podstawy użytkowania API biblioteki Gym.\n",
    "\n",
    "#### Uwaga: Możesz dowolnie modyfikować elementy tego notebooka (wstawiać komórki i zmieniać kod) o ile nie napisano gdzieś inaczej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '=='\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Zainstaluj bibliotekę OpenAI Gym w wersji 0.18.0\n",
    "!pip install gym == 0.18.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import gym"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zad. 1 - Policy iteration + value iteration (10 pkt.)\n",
    "\n",
    "W komórkach poniżej zaimplementuj algorytmy **iteracji polityki** oraz **iteracji wartości**, wyznaczające deterministyczną politykę dla środowiska FrozenLake.\n",
    "\n",
    "Odpowiedź na pytania wykonując odpowiednie eksperymenty (zostaw output odpowiednich komórek na poparcie swoich twierdzeń):\n",
    "- Jak zmiana współczynniku `gamma` wpływa na wynikową politykę?\n",
    "- Jak stochastyczność wpływa na liczbę iteracji potrzebnych do zbiegnięcia obu algorytmów oraz wynikową politykę?\n",
    "\n",
    "#### Uwaga: nie zmieniaj nazwy funkcji `policy_iteration` i `value_iteration`, ani ich argumentów. Nie dopisuj do komórek z funkcjami innego kodu. Może zdefiniować funkcje pomocnicze dla danej funkcji w tej samej komórce (sprawdzarka wyciągnie ze zgłoszonego notebooka wyłącznie komórki zawierającą funkcje `policy_iteration` i `value_iteration` do sprawdzenia, kod w innych komórkach nie będzie widziany przez sprawdzarkę!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Odpowiedzi:\n",
    "1. Współczynnik `gamma` ustawiony w przedziale 0,0-0,1 lub o wartości 1.0 powoduje błędne polityki o złych liczbach iteracji, które powodują przegraną ciągłą przegraną.\n",
    "2. Stochastyczność wpływa na:\n",
    "- Liczba iteracji potrzebnych do zbiegnięcia:\n",
    "  - Stochastyczne — Potrzebujemy więcej iteracji przy większym gamma do zbiegnięcia. Może to wynikać ze względu na otrzebę uwzględnienia prawdopodobieństwa przejść pomiędzy stanami, co każe agentowi myśleć o ryzku związanymi z akcjami.\n",
    "  - Deterministyczne — zbiegają szybciej ze względu na brak potrzeby przewidywania przez agenta ryzyka swoich akcji.\n",
    "- Wynikowa polityka:\n",
    "  - Stochastyczne — Zdaje się ostrożna, agent możliwie uwzględnia ryzyko związane ze ślizganiem się na lodzie. Przez co możliwie, żę będzie rozważać ścieżki, które prowadzą do celu z mniejszym ryzykiem, ale są dłuższe.\n",
    "  - Deterministyczne — Agent może zawsze wybrać najkrótszą ścieżkę do celu ze względu na pewność wykonanych akcji przez agenta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def evaluate_empirically(env, pi, episodes=1000, max_actions=100):\n",
    "  mean_reward = 0\n",
    "  for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    total = 0\n",
    "\n",
    "    for _ in range(max_actions):\n",
    "      state, reward, termination, _ = env.step(pi[state])\n",
    "      total += reward\n",
    "      if termination: break\n",
    "    mean_reward = mean_reward + 1 / (episode + 1) * (total - mean_reward)\n",
    "  return mean_reward"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(P, gamma, delta=0.001):\n",
    "  \"\"\"\n",
    "  Argumenty:\n",
    "      P — model przejścia, gdzie P[s][a] == [(probability, nextstate, reward, done), ...]\n",
    "      gamma — współczynnik dyskontujący\n",
    "      delta — tolerancja warunku stopu\n",
    "  Zwracane wartości:\n",
    "      V — lista o długości len(P) zawierający oszacowane wartość stanu s: V[s]\n",
    "      pi — lista o długości len(P) zawierający wyznaczoną deterministyczną politykę - akcję dla stanu s: pi[s]\n",
    "      i — ilość iteracji algorytmu po wszystkich stanach\n",
    "  \"\"\"\n",
    "  def evaluate():\n",
    "    while True:\n",
    "      score = 0\n",
    "      for state in states:\n",
    "        value = V[state]\n",
    "        action = policy[state]\n",
    "        V[state] = sum(\n",
    "          probability * (reward + gamma * V[next]) for probability, next, reward, _ in P[state][action]\n",
    "        )\n",
    "        score = max(score, abs(value - V[state]))\n",
    "      if score < delta: break\n",
    "\n",
    "  def improve():\n",
    "    is_stable = True\n",
    "    for state in states:\n",
    "      previous = policy[state]\n",
    "      actions = range(len(P[state]))\n",
    "      policy[state] = max(actions, key=lambda action: sum(\n",
    "        probability * (reward + gamma * V[next]) for probability, next, reward, _ in P[state][action]\n",
    "      ))\n",
    "\n",
    "      if previous != policy[state]: is_stable = False\n",
    "    return is_stable\n",
    "\n",
    "  V = [0] * len(P)\n",
    "  policy = [0] * len(P)\n",
    "  states = range(len(P))\n",
    "  iterations = 0\n",
    "\n",
    "  while True:\n",
    "    iterations += 1\n",
    "    evaluate()\n",
    "    is_stable = improve()\n",
    "    if is_stable: break\n",
    "\n",
    "  return V, policy, iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wartości stanów (V):\n",
      "[0.03125, 0.0625, 0.125, 0.0625, 0.0625, 0.0, 0.25, 0.0, 0.125, 0.25, 0.5, 0.0, 0.0, 0.5, 1.0, 0.0]\n",
      "Polityka (pi):\n",
      "[1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0]\n",
      "Liczba iteracji: 7\n",
      "Empiryczny wynik: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "P = env.P\n",
    "gamma = 0.5\n",
    "V, policy, iterations = policy_iteration(P, gamma)\n",
    "\n",
    "print(f\"\"\"\n",
    "Wartości stanów (V):\n",
    "{V}\n",
    "Polityka (pi):\n",
    "{policy}\n",
    "Liczba iteracji: {iterations}\n",
    "Empiryczny wynik: {evaluate_empirically(env, policy)}\n",
    "\"\"\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def value_iteration(P, gamma, delta=0.001):\n",
    "  \"\"\"\n",
    "  Argumenty:\n",
    "      P — model przejścia, gdzie P[s][a] == [(probability, nextstate, reward, done), ...]\n",
    "      gamma — współczynnik dyskontujący\n",
    "      delta — tolerancja warunku stopu\n",
    "  Zwracane wartości:\n",
    "      Q — lista o długości len(P) zawierający listy z oszacowanymi wartościami dla stanu s i akcji a: Q[s][a]\n",
    "      pi — lista o długości len(P) zawierający wyznaczoną deterministyczną politykę - akcję dla stanu s: pi[s]\n",
    "      i — ilość iteracji algorytmu po wszystkich stanach\n",
    "  \"\"\"\n",
    "  def evaluate():\n",
    "    score = 0\n",
    "    for state in states:\n",
    "      for action in range(len(P[state])):\n",
    "        q = Q[state][action]\n",
    "        Q[state][action] = sum([p * (r + gamma * max(Q[s_])) for p, s_, r, _ in P[state][action]])\n",
    "        score = max(score, abs(q - Q[state][action]))\n",
    "    return score\n",
    "\n",
    "  def extract():\n",
    "    return [Q[state].index(max(Q[state])) for state in states]\n",
    "\n",
    "  states = P.keys()\n",
    "  Q = [[0] * len(P[state]) for state in states]\n",
    "  iterations = 0\n",
    "\n",
    "  while True:\n",
    "    iterations += 1\n",
    "    score = evaluate()\n",
    "    if score < delta: break\n",
    "\n",
    "  policy = extract()\n",
    "  return Q, policy, iterations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wartości stanów i akcji (Q):\n",
      "[[0.015625, 0.03125, 0.03125, 0.015625], [0.015625, 0.0, 0.0625, 0.03125], [0.03125, 0.125, 0.03125, 0.0625], [0.0625, 0.0, 0.03125, 0.03125], [0.03125, 0.0625, 0.0, 0.015625], [0.0, 0.0, 0.0, 0.0], [0.0, 0.25, 0.0, 0.0625], [0.0, 0.0, 0.0, 0.0], [0.0625, 0.0, 0.125, 0.03125], [0.0625, 0.25, 0.25, 0.0], [0.125, 0.5, 0.0, 0.125], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.25, 0.5, 0.125], [0.25, 0.5, 1.0, 0.25], [0.0, 0.0, 0.0, 0.0]]\n",
      "Polityka (pi):\n",
      "[1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0]\n",
      "Liczba iteracji: 8\n",
      "Empiryczny wynik: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "P = env.P\n",
    "gamma = 0.5\n",
    "Q, policy, iterations = value_iteration(P, gamma)\n",
    "\n",
    "print(f\"\"\"\n",
    "Wartości stanów i akcji (Q):\n",
    "{Q}\n",
    "Polityka (pi):\n",
    "{policy}\n",
    "Liczba iteracji: {iterations}\n",
    "Empiryczny wynik: {evaluate_empirically(env, policy)}\n",
    "\"\"\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Eksperyment wpływu `gamma`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration:\n",
      "\n",
      "  Gamma: 0.0\n",
      "  Polityka (pi): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0]\n",
      "  Liczba iteracji: 2\n",
      "  Empiryczny wynik: 0.0\n",
      "  \n",
      "\n",
      "  Gamma: 0.1\n",
      "  Polityka (pi): [1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0]\n",
      "  Liczba iteracji: 7\n",
      "  Empiryczny wynik: 1.0\n",
      "  \n",
      "\n",
      "  Gamma: 0.5\n",
      "  Polityka (pi): [1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0]\n",
      "  Liczba iteracji: 7\n",
      "  Empiryczny wynik: 1.0\n",
      "  \n",
      "\n",
      "  Gamma: 0.9\n",
      "  Polityka (pi): [1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0]\n",
      "  Liczba iteracji: 7\n",
      "  Empiryczny wynik: 1.0\n",
      "  \n",
      "\n",
      "  Gamma: 0.99\n",
      "  Polityka (pi): [1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0]\n",
      "  Liczba iteracji: 7\n",
      "  Empiryczny wynik: 1.0\n",
      "  \n",
      "\n",
      "  Gamma: 1\n",
      "  Polityka (pi): [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "  Liczba iteracji: 8\n",
      "  Empiryczny wynik: 0.0\n",
      "  \n",
      "Value Iteration:\n",
      "\n",
      "  Gamma: 0.0\n",
      "  Polityka (pi): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0]\n",
      "  Liczba iteracji: 2\n",
      "  Empiryczny wynik: 0.0\n",
      "  \n",
      "\n",
      "  Gamma: 0.1\n",
      "  Polityka (pi): [0, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0]\n",
      "  Liczba iteracji: 5\n",
      "  Empiryczny wynik: 0.0\n",
      "  \n",
      "\n",
      "  Gamma: 0.5\n",
      "  Polityka (pi): [1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0]\n",
      "  Liczba iteracji: 8\n",
      "  Empiryczny wynik: 1.0\n",
      "  \n",
      "\n",
      "  Gamma: 0.9\n",
      "  Polityka (pi): [1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0]\n",
      "  Liczba iteracji: 8\n",
      "  Empiryczny wynik: 1.0\n",
      "  \n",
      "\n",
      "  Gamma: 0.99\n",
      "  Polityka (pi): [1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0]\n",
      "  Liczba iteracji: 8\n",
      "  Empiryczny wynik: 1.0\n",
      "  \n",
      "\n",
      "  Gamma: 1\n",
      "  Polityka (pi): [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "  Liczba iteracji: 8\n",
      "  Empiryczny wynik: 0.0\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "P = env.P\n",
    "gammas = [0.0, 0.1, 0.5, 0.9, 0.99, 1]\n",
    "\n",
    "print(\"Policy Iteration:\")\n",
    "for gamma in gammas:\n",
    "  _, policy, iterations = policy_iteration(P, gamma)\n",
    "  print(f\"\"\"\n",
    "  Gamma: {gamma}\n",
    "  Polityka (pi): {policy}\n",
    "  Liczba iteracji: {iterations}\n",
    "  Empiryczny wynik: {evaluate_empirically(env, policy)}\n",
    "  \"\"\")\n",
    "\n",
    "print(\"Value Iteration:\")\n",
    "for gamma in gammas:\n",
    "  _, policy, iterations = value_iteration(P, gamma)\n",
    "  print(f\"\"\"\n",
    "  Gamma: {gamma}\n",
    "  Polityka (pi): {policy}\n",
    "  Liczba iteracji: {iterations}\n",
    "  Empiryczny wynik: {evaluate_empirically(env, policy)}\n",
    "  \"\"\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Eksperyment środowiska deterministycznego kontra stochastycznego."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration:\n",
      "\n",
      "  Gamma: 0.0\n",
      "  Środowiska deterministyczne\n",
      "  Empiryczny wynik: 0.0\n",
      "  Liczba iteracji: 2\n",
      "  Polityka: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "  Środowiska stochastycznego\n",
      "  Empiryczny wynik: 0.0\n",
      "  Liczba iteracji: 2\n",
      "  Polityka: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "  \n",
      "\n",
      "  Gamma: 0.1\n",
      "  Środowiska deterministyczne\n",
      "  Empiryczny wynik: 1.0\n",
      "  Liczba iteracji: 7\n",
      "  Polityka: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "  Środowiska stochastycznego\n",
      "  Empiryczny wynik: 0.47400000000000037\n",
      "  Liczba iteracji: 6\n",
      "  Polityka: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "  \n",
      "\n",
      "  Gamma: 0.5\n",
      "  Środowiska deterministyczne\n",
      "  Empiryczny wynik: 1.0\n",
      "  Liczba iteracji: 7\n",
      "  Polityka: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "  Środowiska stochastycznego\n",
      "  Empiryczny wynik: 0.4430000000000003\n",
      "  Liczba iteracji: 5\n",
      "  Polityka: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "  \n",
      "\n",
      "  Gamma: 0.9\n",
      "  Środowiska deterministyczne\n",
      "  Empiryczny wynik: 1.0\n",
      "  Liczba iteracji: 7\n",
      "  Polityka: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "  Środowiska stochastycznego\n",
      "  Empiryczny wynik: 0.7260000000000002\n",
      "  Liczba iteracji: 6\n",
      "  Polityka: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "  \n",
      "\n",
      "  Gamma: 0.99\n",
      "  Środowiska deterministyczne\n",
      "  Empiryczny wynik: 1.0\n",
      "  Liczba iteracji: 7\n",
      "  Polityka: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "  Środowiska stochastycznego\n",
      "  Empiryczny wynik: 0.7589999999999997\n",
      "  Liczba iteracji: 7\n",
      "  Polityka: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "  \n",
      "\n",
      "  Gamma: 1\n",
      "  Środowiska deterministyczne\n",
      "  Empiryczny wynik: 0.0\n",
      "  Liczba iteracji: 8\n",
      "  Polityka: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "  Środowiska stochastycznego\n",
      "  Empiryczny wynik: 0.7430000000000011\n",
      "  Liczba iteracji: 7\n",
      "  Polityka: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "  \n",
      "Value Iteration:\n",
      "\n",
      "  Gamma: 0.0\n",
      "  Środowiska deterministyczne\n",
      "  Empiryczny wynik: 0.0\n",
      "  Liczba iteracji: 2\n",
      "  Polityka: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "  Środowiska stochastycznego\n",
      "  Empiryczny wynik: 0.0\n",
      "  Liczba iteracji: 2\n",
      "  Polityka: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "  \n",
      "\n",
      "  Gamma: 0.1\n",
      "  Środowiska deterministyczne\n",
      "  Empiryczny wynik: 0.0\n",
      "  Liczba iteracji: 5\n",
      "  Polityka: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "  Środowiska stochastycznego\n",
      "  Empiryczny wynik: 0.0\n",
      "  Liczba iteracji: 3\n",
      "  Polityka: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "  \n",
      "\n",
      "  Gamma: 0.5\n",
      "  Środowiska deterministyczne\n",
      "  Empiryczny wynik: 1.0\n",
      "  Liczba iteracji: 8\n",
      "  Polityka: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "  Środowiska stochastycznego\n",
      "  Empiryczny wynik: 0.44200000000000045\n",
      "  Liczba iteracji: 6\n",
      "  Polityka: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "  \n",
      "\n",
      "  Gamma: 0.9\n",
      "  Środowiska deterministyczne\n",
      "  Empiryczny wynik: 1.0\n",
      "  Liczba iteracji: 8\n",
      "  Polityka: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "  Środowiska stochastycznego\n",
      "  Empiryczny wynik: 0.7310000000000001\n",
      "  Liczba iteracji: 21\n",
      "  Polityka: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "  \n",
      "\n",
      "  Gamma: 0.99\n",
      "  Środowiska deterministyczne\n",
      "  Empiryczny wynik: 1.0\n",
      "  Liczba iteracji: 8\n",
      "  Polityka: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "  Środowiska stochastycznego\n",
      "  Empiryczny wynik: 0.7389999999999999\n",
      "  Liczba iteracji: 82\n",
      "  Polityka: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "  \n",
      "\n",
      "  Gamma: 1\n",
      "  Środowiska deterministyczne\n",
      "  Empiryczny wynik: 0.0\n",
      "  Liczba iteracji: 8\n",
      "  Polityka: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "  Środowiska stochastycznego\n",
      "  Empiryczny wynik: 0.7319999999999993\n",
      "  Liczba iteracji: 117\n",
      "  Polityka: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "env_deterministic = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "env_stochastic = gym.make('FrozenLake-v0', is_slippery=True)\n",
    "P_deterministic = env_deterministic.P\n",
    "P_stochastic = env_stochastic.P\n",
    "\n",
    "gammas = [0.0, 0.1, 0.5, 0.9, 0.99, 1]\n",
    "\n",
    "print(\"Policy Iteration:\")\n",
    "for gamma in gammas:\n",
    "  _, deterministic_policy, deterministic_iterations = policy_iteration(P_deterministic, gamma)\n",
    "  _, stochastic_policy, stochastic_iterations = policy_iteration(P_stochastic, gamma)\n",
    "  print(f\"\"\"\n",
    "  Gamma: {gamma}\n",
    "  Środowiska deterministyczne\n",
    "  Empiryczny wynik: {evaluate_empirically(env_deterministic, deterministic_policy)}\n",
    "  Liczba iteracji: {deterministic_iterations}\n",
    "  Polityka: {policy}\n",
    "  Środowiska stochastycznego\n",
    "  Empiryczny wynik: {evaluate_empirically(env_stochastic, stochastic_policy)}\n",
    "  Liczba iteracji: {stochastic_iterations}\n",
    "  Polityka: {policy}\n",
    "  \"\"\")\n",
    "print(\"Value Iteration:\")\n",
    "for gamma in gammas:\n",
    "  _, deterministic_policy, deterministic_iterations = value_iteration(P_deterministic, gamma)\n",
    "  _, stochastic_policy, stochastic_iterations = value_iteration(P_stochastic, gamma)\n",
    "  print(f\"\"\"\n",
    "  Gamma: {gamma}\n",
    "  Środowiska deterministyczne\n",
    "  Empiryczny wynik: {evaluate_empirically(env_deterministic, deterministic_policy)}\n",
    "  Liczba iteracji: {deterministic_iterations}\n",
    "  Polityka: {policy}\n",
    "  Środowiska stochastycznego\n",
    "  Empiryczny wynik: {evaluate_empirically(env_stochastic, stochastic_policy)}\n",
    "  Liczba iteracji: {stochastic_iterations}\n",
    "  Polityka: {policy}\n",
    "  \"\"\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zad. 2 - Monte Carlo (10 pkt.)\n",
    "W komórce poniżej zaimplementuj metodę **On-policy Monte Carlo** dla polityki epsilon-greedy.\n",
    "Zakładamy, że model przejść nie jest w tym wypadku dla nas dostępny,\n",
    "dlatego możesz używać wyłącznie metod `env.reset()` i `env.step()`\n",
    "w swojej implementacji, w celu wygenerowania nowego epizodu.\n",
    "\n",
    "- Zaproponuj warunek stopu dla swojej implementacji.\n",
    "- Jaki jest wpływ epsilony na działanie algorytmu?\n",
    "- Jaka prosta modyfikacja nagród środowiska przyśpieszyłaby odkrywanie dobrej polityki? Zmodyfikuj env.P i zademonstruj.\n",
    "\n",
    "Tip: z racji, że env.P jest dostępne, możesz porównać wyniki `on_policy_eps_greedy_monte_carlo` ze wynikami `value_iteration`. \n",
    "\n",
    "#### Uwaga: nie zmieniaj nazwy funkcji `on_policy_eps_greedy_monte_carlo`, ani jej pierwszych argumentów (możesz dodać nowe argumenty z wartościami domyślnymi). Nie dopisuj do komórki z funkcją innego kodu. Może zdefiniować funkcje pomocnicze dla funkcji w tej samej komórce (sprawdzarka wyciągnie ze zgłoszonego notebooka wyłącznie komórkę zawierającą funkcję `on_policy_eps_greedy_monte_carlo` do sprawdzenia, kod w innych komórkach nie będzie widziany przez sprawdzarkę!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Odpowiedź:\n",
    "1. Propozycją implementacji warunku stopu jest maksymalna liczba epizodów.\n",
    "2. Epsilon wpływa na to, jak często algorytm eksploruje środowisko. Im większa epsilon, tym częściej eksploruje.\n",
    "  - Przy mniejszych wartościach agent będzie częściej podejmował akcji według założonej polityki i rzadziej eksplorował środowisko. Co może skutkować tym, że algorytm będzie potrzebował mniej epizodów do pełnej polityki, jednak istnieje ryzyko, że algorytm utknie w jakiejś lokalnej optymalności.\n",
    "  - Przy większych wartościach agent będzie częściej eksplorował środowisko i rzadziej podejmował akcje według założonej polityki. Co może skutkować tym, że algorytm będzie potrzebował więcej epizodów do odkrycia optymalnej polityki.\n",
    "3. Modyfikacja nagród polega na tym, że istnieje kara za każdą wpadniętą dziurę. Dzięki temu algorytm szybciej znajduje optymalną politykę."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "from gym.envs.toy_text import FrozenLakeEnv\n",
    "\n",
    "def modify_rewards(env):\n",
    "  P = env.P\n",
    "  map_ = b''.join(item for sublist in env.desc for item in sublist)\n",
    "\n",
    "  holes = [state for (state, type_) in enumerate(map_) if type_ is ord(b'H')]\n",
    "  goals = [state for (state, type_) in enumerate(map_) if type_ is ord(b'G')]\n",
    "\n",
    "  for state in P:\n",
    "    for action in P[state]:\n",
    "      for (i, (probability, next_state, reward, termination)) in enumerate(P[state][action]):\n",
    "        if next_state in holes:\n",
    "          P[state][action][i] = (probability, next_state, -0.05, termination)\n",
    "        elif state != next_state and next_state in goals:\n",
    "          P[state][action][i] = (probability, next_state, 1, termination)\n",
    "        elif state != next_state:\n",
    "          P[state][action][i] = (probability, next_state, 0, termination)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "def on_policy_eps_greedy_monte_carlo(env, eps, gamma, max_episodes=10000):\n",
    "  \"\"\"\n",
    "  Argumenty:\n",
    "      env — środowisko implementujące metody `reset()` oraz `step(action)`\n",
    "      eps — współczynnik eksploracji\n",
    "      gamma — współczynnik dyskontujący\n",
    "  Zwracane wartości:\n",
    "      Q — lista o długości len(P) zawierający listy z oszacowanymi wartościami dla stanu s i akcji a: Q[s][a]\n",
    "      pi — lista o długości len(P) zawierający wyznaczoną deterministyczną (zachłanną) politykę - akcję dla stanu s: pi[s]\n",
    "      i — ilość epizodów wygenerowanych przez algorytm\n",
    "  \"\"\"\n",
    "  def generate_episodes():\n",
    "    def e_greedy(state):\n",
    "      if random.uniform(0, 1) < eps: return env.action_space.sample()\n",
    "      return policy[state]\n",
    "\n",
    "    nonlocal iterations\n",
    "    while (iterations := iterations + 1) < max_episodes:\n",
    "      state = env.reset()\n",
    "      episode = []\n",
    "\n",
    "      while True:\n",
    "        action = e_greedy(state)\n",
    "        (next_state, reward, termination, _) = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        if termination: break\n",
    "      yield episode\n",
    "\n",
    "  visits = defaultdict(int)\n",
    "  def evaluate(episode):\n",
    "    returns = 0\n",
    "\n",
    "    states = [state for (state, _, _) in episode]\n",
    "    for (epoch, (state, action, reward)) in reversed(list(enumerate(episode))):\n",
    "      returns = gamma * returns + reward\n",
    "\n",
    "      if state not in states[:epoch]:\n",
    "        visits[(state, action)] += 1\n",
    "        Q[state][action] += (returns - Q[state][action]) / visits[(state, action)]\n",
    "\n",
    "  def extract():\n",
    "    return [Q[state].index(max(Q[state])) for state in range(len(Q))]\n",
    "\n",
    "  P = env.P\n",
    "  states = range(len(P))\n",
    "  Q = [[0] * len(P[s]) for s in states]\n",
    "  policy = extract()\n",
    "  iterations = 0\n",
    "\n",
    "  episode_it = generate_episodes()\n",
    "  while episode := next(episode_it, None):\n",
    "    evaluate(episode)\n",
    "    policy = extract()\n",
    "\n",
    "  return Q, policy, iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wartości stanów i akcji (Q):\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001B[41mG\u001B[0m\n",
      "[0.12 0.1  0.11 0.1 ]\n",
      "[0.04 0.05 0.03 0.09]\n",
      "[0.06 0.08 0.08 0.05]\n",
      "[0.02 0.03 0.01 0.06]\n",
      "[0.14 0.08 0.07 0.06]\n",
      "[0. 0. 0. 0.]\n",
      "[ 0.11  0.07  0.12 -0.01]\n",
      "[0. 0. 0. 0.]\n",
      "[0.09 0.13 0.12 0.19]\n",
      "[0.18 0.3  0.22 0.15]\n",
      "[0.35 0.3  0.23 0.12]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0.21 0.33 0.45 0.29]\n",
      "[0.46 0.69 0.64 0.57]\n",
      "[0. 0. 0. 0.]\n",
      "\n",
      "Polityka (pi):\n",
      "[0, 3, 2, 3, 0, 0, 2, 0, 3, 1, 0, 0, 0, 2, 1, 0]\n",
      "Liczba epizodów: 100000\n",
      "Wynik empiryczny: 0.69\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env: FrozenLakeEnv = gym.make('FrozenLake-v0', is_slippery=True)\n",
    "epsilon = 0.25\n",
    "gamma = 0.99\n",
    "max_episodes = 100000\n",
    "newline = '\\n'\n",
    "modify_rewards(env)\n",
    "Q, policy, iterations = on_policy_eps_greedy_monte_carlo(env, epsilon, gamma, max_episodes)\n",
    "print(\"Wartości stanów i akcji (Q):\")\n",
    "import numpy as np\n",
    "env.render()\n",
    "Q = np.round_(Q, decimals=2)\n",
    "print(*Q, sep='\\n')\n",
    "print(f\"\"\"\n",
    "Polityka (pi):\n",
    "{policy}\n",
    "Liczba epizodów: {iterations}\n",
    "Wynik empiryczny: {evaluate_empirically(env, policy):.2f}\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
