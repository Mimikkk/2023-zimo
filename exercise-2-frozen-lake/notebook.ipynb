{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zaawansowane Metody Inteligencji Obliczeniowej\n",
    "# Zadanie domowe 2\n",
    "### Prowadzący: Michał Kempka, Marek Wydmuch\n",
    "### Autor: twoje imię i nazwisko + numer indeksu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wprowadzenie\n",
    "\n",
    "Całe zadanie jest oparte o różne wersje środowiska `FrozenLake` ze znanej biblioteki OpenAI Gym (https://gym.openai.com), która agreguje różnego rodzaju środowiska pod postacią jednego zunifikowanego API.\n",
    "\n",
    "Zapoznaj się z opisem środowiska (https://gym.openai.com/envs/FrozenLake-v0), a następnie zapoznaj się z kodem poniżej. Pokazuje on podstawy użytkowania API biblioteki Gym.\n",
    "\n",
    "#### Uwaga: Możesz dowolnie modyfikować elementy tego notebooka (wstawiać komórki i zmieniać kod) o ile nie napisano gdzieś inaczej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Zainstaluj bibliotekę OpenAI Gym w wersji 0.18.0\n",
    "!pip install gym==0.18.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zaimportuj środowisko FrozenLake z OpenAI Gym\n",
    "from gym.envs.toy_text.frozen_lake import FrozenLakeEnv \n",
    "\n",
    "# Stwórzmy deterministyczne (`is_slippper=False`) środowisko w oparciu o jedną z zpredefiniowanych map (`map_name=\"4x4\"`)\n",
    "env = FrozenLakeEnv(map_name=\"4x4\", is_slippery=False) \n",
    "\n",
    "# Po stworzeniu środowiska musimy je zresetować \n",
    "env.reset()\n",
    "# W każdym momencie możemy wyświetlić stan naszego środowiska przy użyciu fukcji `render`\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Najważniejsze pola środowiska, na potrzeby tego zadania załóżmy, że mamy dostęp do nich wszystkich \n",
    "# (oczywiście dla niektórych środowisk w OpenAI Gym tak nie jest)\n",
    "print(\"Przestrzeń akcji: \", env.action_space) # Akcje od 0 do 3: LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3\n",
    "print(\"Przestrzeń obserwacji: \", env.observation_space) # Stany od 0 do 15\n",
    "print(\"Opis środowiska (mapa):\")\n",
    "print(env.desc)\n",
    "print(\"Model przejść w środowisku:\")\n",
    "pprint(env.P) # gdzie P[s][a] == [(probability, nextstate, reward, done), ...]\n",
    "print(\"Aktualny stan: \", env.s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nasz agent może wejść w interakcje ze środowiskiem  poprzez wywołanie funkcji `step(action)`, \n",
    "# gdzie `action` to jedna z możliwych akcji (int od 0 do env.action_space.n - 1)\n",
    "s = env.reset() # `reset()` zwraca początkowy stan\n",
    "env.render()\n",
    "for i in range(5):\n",
    "    # Wybierzmy losową akcje\n",
    "    random_a = env.action_space.sample() \n",
    "    # `step(action)` zwraca nowy stan (`s`), nagrodę (`r`), informację czy stan jest terminalny (`term`) \n",
    "    # oraz dodatkowe informacje, które pomijamy\n",
    "    # w tym wypadku nowy stan to jedynie id, ale dla innych środowisk może być to innym typ reprezentujący obserwację\n",
    "    s, r, term, _ = env.step(random_a) \n",
    "    env.render()\n",
    "    if term:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zad. 1 - Policy iteration + value iteration (10 pkt.)\n",
    "\n",
    "W komórkach poniżej zaimplementuj algorytmy **iteracji polityki** oraz **iteracji wartości**, wyznaczające deterministyczną politykę dla środowiska FrozenLake.\n",
    "\n",
    "Odpowiedź na pytania wykonując odpowiednie eksperymenty (zostaw output odpowiednich komórek na poparcie swoich twierdzeń):\n",
    "- Jak zmiana współczynniku `gamma` wpływa na wynikową politykę?\n",
    "- Jak stochastyczność wpływa na liczbę iteracji potrzebnych do zbiegnięcia obu algorytmów oraz wynikową politykę?\n",
    "\n",
    "#### Uwaga: nie zmieniaj nazwy funkcji `policy_iteration` i `value_iteration`, ani ich argumentów. Nie dopisuj do komórek z funkcjami innego kodu. Może zdefiniować funkcje pomocnicze dla danej funkcji w tej samej komórce (sprawdzarka wyciągnie ze zgłoszonego notebooka wyłącznie komórki zawierającą funkcje `policy_iteration` i `value_iteration` do sprawdzenia, kod w innych komórkach nie będzie widziany przez sprawdzarkę!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Odpowiedzi: Miejsce na Twoje odpowiedzi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(P, gamma, delta=0.001):\n",
    "    \"\"\"\n",
    "    Argumenty:\n",
    "        P - model przejścia, gdzie P[s][a] == [(probability, nextstate, reward, done), ...]\n",
    "        gamma - współczynnik dyskontujący\n",
    "        delta - tolerancja warunku stopu\n",
    "    Zwracane wartości:\n",
    "        V - lista o długości len(P) zawierający oszacowane wartość stanu s: V[s]\n",
    "        pi - lista o długości len(P) zawierający wyznaczoną deterministyczną politykę - akcję dla stanu s: pi[s]\n",
    "        i - ilość iteracji algorytmu po wszystkich stanach\n",
    "    \"\"\"\n",
    "    V = [0] * len(P)\n",
    "    pi = [0] * len(P)\n",
    "    i = 0\n",
    "    \n",
    "    # Miejsce na twoją implementację\n",
    "    \n",
    "    return V, pi, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(P, gamma, delta=0.001):\n",
    "    \"\"\"\n",
    "    Argumenty:\n",
    "        P - model przejścia, gdzie P[s][a] == [(probability, nextstate, reward, done), ...]\n",
    "        gamma - współczynnik dyskontujący\n",
    "        delta - tolerancja warunku stopu\n",
    "    Zwracane wartości:\n",
    "        Q - lista o długości len(P) zawierający listy z oszacowanymi wartościami dla stanu s i akcji a: Q[s][a]\n",
    "        pi - lista o długości len(P) zawierający wyznaczoną deterministyczną politykę - akcję dla stanu s: pi[s]\n",
    "        i - ilość iteracji algorytmu po wszystkich stanach\n",
    "    \"\"\"\n",
    "    pi = [0] * len(P)\n",
    "    Q = [[0] * len(P[s]) for s in P.keys()]\n",
    "    i = 0\n",
    "    \n",
    "    # Miejsce na twoją implementację\n",
    "    \n",
    "    return Q, pi, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykładowy kod do testowania zaimplementowanych metod\n",
    "\n",
    "# Zaimportuj generator map dla środowiska FrozenLake z OpenAI Gym\n",
    "from gym.envs.toy_text.frozen_lake import generate_random_map\n",
    "\n",
    "# Wygeneruj losową mapę jeziora o zadanym rozmiarze (`size=`)\n",
    "lake_map = generate_random_map(size=8)\n",
    "\n",
    "# Stwórz środowisko w oparciu o wygenerowaną mapę, \n",
    "# sprawdz deterministyczną (`is_slippery=False`) jak i stochastyczną wersję środowiska (`is_slippery=True`)\n",
    "env = FrozenLakeEnv(desc=lake_map, is_slippery=True)\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V, pi1, i = policy_iteration(env.P, 0.9)\n",
    "Q, pi2, i = value_iteration(env.P, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wprowadzmy teraz funkcję, które empirycznie zewauluje naszą politykę\n",
    "# po prostu rozgrywając odpowiednią liczbę episodów zgodnie z naszą polityką.\n",
    "def evaluate_empiricaly(env, pi, episodes=1000, max_actions=100):\n",
    "    mean_r = 0\n",
    "    for e in range(episodes):\n",
    "        s = env.reset()\n",
    "        total_r = 0\n",
    "        for _ in range(max_actions): # Na wypadek polityki, która nigdy nie dojdzie od stanu terminalnego\n",
    "            s, r, final, _ = env.step(pi[s])\n",
    "            total_r += r\n",
    "            if final:\n",
    "                break\n",
    "        mean_r = mean_r + 1/(e + 1) * (total_r - mean_r)\n",
    "    return mean_r       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_empiricaly(env, pi1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zad. 2 - Monte Carlo (10 pkt.)\n",
    "W komórce poniżej zaimplementuj metodę **On-policy Monte Carlo** dla polityki epsilon-greedy.\n",
    "Zakładamy, że model przejść nie jest w tym wypadku dla nas dostępny,\n",
    "dlatego możesz używać wyłącznie metod `env.reset()` i `env.step()`\n",
    "w swojej implementacji, w celu wygenerowania nowego epizodu.\n",
    "\n",
    "- Zaproponuj warunek stopu dla swojej implementacji.\n",
    "- Jaki jest wpływ epsilony na działanie algorytmu?\n",
    "- Jaka prosta modyfikacja nagród środowiska przyśpieszyłaby odkrywanie dobrej polityki? Zmodyfikuj env.P i zademonstruj.\n",
    "\n",
    "Tip: z racji, że env.P jest dostępne, możesz porównać wyniki `on_policy_eps_greedy_monte_carlo` ze wynikami `value_iteration`. \n",
    "\n",
    "#### Uwaga: nie zmieniaj nazwy funkcji `on_policy_eps_greedy_monte_carlo`, ani jej pierwszych argumentów (możesz dodać nowe argumenty z wartościami domyślnymi). Nie dopisuj do komórki z funkcją innego kodu. Może zdefiniować funkcje pomocnicze dla funkcji w tej samej komórce (sprawdzarka wyciągnie ze zgłoszonego notebooka wyłącznie komórkę zawierającą funkcję `on_policy_eps_greedy_monte_carlo` do sprawdzenia, kod w innych komórkach nie będzie widziany przez sprawdzarkę!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Odpowiedź: Miejsce na Twoje odpowiedzi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_policy_eps_greedy_monte_carlo(env, eps, gamma):\n",
    "    \"\"\"\n",
    "    Argumenty:\n",
    "        env - środowisko implementujące metody `reset()` oraz `step(action)`\n",
    "        eps - współczynnik eksploracji\n",
    "        gamma - współczynnik dyskontujący\n",
    "    Zwracane wartości:\n",
    "        Q - lista o długości len(P) zawierający listy z oszacowanymi wartościami dla stanu s i akcji a: Q[s][a]\n",
    "        pi - lista o długości len(P) zawierający wyznaczoną deterministyczną (zachłanną) politykę - akcję dla stanu s: pi[s]\n",
    "        i - ilość epizodów wygenerowanych przez algorytm\n",
    "    \"\"\"\n",
    "    pi = [0] * len(env.P)\n",
    "    Q = [[0] * len(env.P[s]) for s in env.P.keys()]\n",
    "    i = 0\n",
    "    \n",
    "    # Miejsce na twoją implementację\n",
    "    \n",
    "    return Q, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
