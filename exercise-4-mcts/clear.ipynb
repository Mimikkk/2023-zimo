{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zaawansowane Metody Inteligencji Obliczeniowej\n",
    "# Zadanie domowe 4 (30 pkt.)\n",
    "### Prowadzący: Michał Kempka, Marek Wydmuch\n",
    "### Autor: twoje imię i nazwisko + numer indeksu, grupa"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wprowadzenie do Zadania 1\n",
    "\n",
    "Zadanie będzie polegało na implementacji algorytmu Deep Deterministic Policy Gradient (DDPG) dla środowiska `LunarLanderContinuous` ~~OpenAI Gym~~ Gymnasium (https://gymnasium.farama.org/).\n",
    "\n",
    "#### Uwaga: Możesz dowolnie modyfikować elementy tego notebooka (wstawiać komórki i zmieniać kod) o ile nie napisano gdzieś inaczej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zainstaluj bibliotekę OpenAI Gym i PyTorch\n",
    "!pip install \"gymnasium[box2d]==0.28\" torch matplotlib numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zobacz jak działają środowiska na przykładzie 1000 kroków\n",
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"LunarLanderContinuous-v2\")\n",
    "\n",
    "observation = env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, term, trunc, _ = env.step(action)\n",
    "    done = term or trunc\n",
    "\n",
    "    if done:\n",
    "        observation = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `LunarLanderContinuous` to wersja środowiska `LunarLander` z ciągłymi akcjami\n",
    "# Zobacz jakim typem są obserwacje (reprezentacja stanu) oraz jaka jest przestrzeń akcji w obu wersjach:\n",
    "env_dis = gym.make(\"LunarLander-v2\")\n",
    "env_con = gym.make(\"LunarLanderContinuous-v2\")\n",
    "\n",
    "print(\"Przestrzeń stanów:\", env_dis.observation_space, env_con.observation_space)\n",
    "print(\"Rozmiar wektora reprezntującego stany:\", env_dis.observation_space.shape, env_con.observation_space.shape)\n",
    "print(\"Przestrzeń akcji:\", env_dis.action_space, env_con.action_space)\n",
    "# Jak widać, przestrzeń akcji zmieniła się z dyskretnej o wielkości 4 na wektor 2 liczb z zakresu od -1 do 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interface agenta\n",
    "\n",
    "Wszystkie implementacje będą w oparciu o klasę `Agent` przedstawioną poniżej (identyczna jak w `homework-2-qlearning.ipynb`). Powinna ona implementować dwie metody: 1) `process_transition`, która będzie wykorzystywana tylko podczas uczenia by przekazać do agenta krotkę zawierającą obserwacje, podjętą akcję, nagrodę i następujący stan oraz informację czy jest on terminalny. 2) `get_action` która na podstawie obserwacji zwraca akcję, dodatkowy argument informuję Agenta czy jest w aktualnie podczas uczenia czy ewaluacji.\n",
    "\n",
    "Poniżej znajdziesz też funkcję `run_agent_in_env` (również jest taka sama jak w `homework-2-qlearning.ipynb`), która korzysta z opisanego wyżej obiektu agenta w celu jego uczenia lub ewaluacji (podobną będziemy używać my podczas sprawdzania wszych rozwiązań). Możecie ją modyfikować jeśli widzicie taką potrzebę. Dopisaliśmy do tej funkcji rysowanie wykresu nagród z minionych epizodów, który uaktualnia się co ich zadaną liczbę."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        self.observation_space = env.observation_space\n",
    "        self.action_space = env.action_space\n",
    "    \n",
    "    def process_transition(self, observation, action, reward, next_observation, done):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def get_action(self, observation, learning):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def plot_rewards(rewards):\n",
    "    plt.figure(figsize=(14,6))\n",
    "    plt.plot(rewards)\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    \n",
    "def run_agent_in_env(env, agent, episodes, learning=False, plot=False, plot_interval=1000):\n",
    "    rewards = []\n",
    "    for episode in range(episodes):\n",
    "        observation = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done :\n",
    "            # Zapytajmy agenta o akcje dla aktualnego stanu\n",
    "            action = agent.get_action(observation, learning)\n",
    "            \n",
    "            # Wykonajmy akcje\n",
    "            next_observation, reward, term, trunc, _ = env.step(action)\n",
    "            done = term or trunc\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Jeśli się uczymy, przekażmy przejście do agenta\n",
    "            if learning:\n",
    "                agent.process_transition(observation, action, reward, next_observation, done)\n",
    "            \n",
    "            observation = next_observation\n",
    "        rewards.append(total_reward)\n",
    "        \n",
    "        # Wyświetl na wykresie nagrody otrzymane po kolei w epizodach\n",
    "        if plot and episode % plot_interval == 0:\n",
    "            plot_rewards(rewards)\n",
    "    return rewards    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstracja na przykładzie losowego agenta\n",
    "env = gym.make(\"LunarLanderContinuous-v2\")\n",
    "\n",
    "class RandomAgent(Agent):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def process_transition(self, observation, action, reward, next_observation, done):\n",
    "        pass\n",
    "        \n",
    "    def get_action(self, observation, learning):\n",
    "        return self.action_space.sample()\n",
    "    \n",
    "random_agent = RandomAgent(env)\n",
    "rewards = run_agent_in_env(env, random_agent, 1000, plot=True, plot_interval=50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zad 4.1 - Deep Deterministic Policy Gradient (10pkt)\n",
    "\n",
    "W komórkach poniżej zaimplementuj Deep Deterministic Policy Gradient dla środowiska `LunarLanderContinuous` z eksploracją opartą o dodawanie szumu Gaussowskiego, a jako aproksymatorów użyj prostych sieci neuronowych zaimplementowanych z wykorzystaniem biblioteki PyTroch.\n",
    "\n",
    "Uwagi:\n",
    "- Dobra polityka powina mieć średnią bliską 100, bardzo dobra bliską 200.\n",
    "- Uczenie nie powinno trwać więcej niż 10-15 minut na CPU (możesz oczywiście użyć GPU jeśli masz taką możliwość, ale zadbaj by domyślnie Twoja implemnetacja działałą na CPU).\n",
    "- Postaraj się dobrać i ustawić w `__init__` takie parametry (w tym architektuę sieci neuronowej) by Twój Agent uczył się i działał jak najlepiej. Są one pod Twoją kontrolą w klasie, więc możesz je odpowiednio kontrolować w trakcie uczenia.\n",
    "- Założona architektura nie jest potrzebna do rozwiązania tego problemu, zacznij od prostego aproksymatora (np. liniowego albo sieci z dwoma warstwami) i stopniowo zwiększaj jego złożoność.\n",
    "\n",
    "#### Uwaga: nie zmieniaj nazwy klasy `DDPGAgent`, ani pozycyjnych (tych bez wartości domyślnych) argumentów jej metod, możesz dopisywać argumenty z wartościami domyślnymi oraz oczywiście pomocnicze meotdy do klasy. Nie dopisuj do komórki z klasą innego kodu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGAgent(Agent):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        # Zainicjalizuj tutaj swojego agenta\n",
    "    \n",
    "    def process_transition(self, observation, action, reward, next_observation, done):\n",
    "        # Miejsce na Twoją implementację\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_action(self, observation, learning):\n",
    "        # Miejsce na Twoją implementację\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uczenie/testowania agenta\n",
    "env = gym.make(\"LunarLanderContinuous-v2\")\n",
    "agent = DDPGAgent(env)\n",
    "rewards = run_agent_in_env(env, agent, 10000, learning=True, plot=True, plot_interval=250)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wprowadzenie do Zadania 4.2\n",
    "\n",
    "Zadanie będzie polegało na implementacji algorytmu Monte Carlo Tree Search (MCTS) dla środowiska `Connect Four` z biblioteki DeepMind OpenSpiel (https://github.com/deepmind/open_spiel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zainstaluj bibliotekę DeepMind OpenSpiel (tylko Linux i MacOS)\n",
    "!pip install open_spiel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Środowisko `Connect Four`\n",
    "\n",
    "Poniższy kod demonstruje działanie środowiska `Connect Four` (https://en.wikipedia.org/wiki/Connect_Four), gry dla dwóch graczy, deterministycznej z pełną informacją."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zobacz jak działają środowisko na przykładzie jednej gry\n",
    "import random\n",
    "import pyspiel\n",
    "import numpy as np\n",
    "\n",
    "game = pyspiel.load_game(\"connect_four\")\n",
    "state = game.new_initial_state()\n",
    "\n",
    "while not state.is_terminal():\n",
    "    print('Tura gracza:', state.current_player())\n",
    "    print('Ruch:', state.move_number())\n",
    "    print('Stan:'),\n",
    "    print(state.observation_string())\n",
    "    print('Dopuszczalne akcje:', state.legal_actions())\n",
    "    a = np.random.choice(state.legal_actions()) # Wybieramy losową z dopuszczalnych akcji\n",
    "    print('Wybrana akcja:', a)\n",
    "    state.apply_action(a) # Stosujemy akcję\n",
    "    print('-------')\n",
    "print('Koniec gry, wyniki:', state.player_reward(0), ':', state.player_reward(1)) # albo state.rewards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obiekt State zawiera kilka innych przydatnych metod i pól\n",
    "[a for a in dir(state) if '__' not in a] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Np. historię akcji:\n",
    "[(a.player, a.action) for a in state.full_history()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Albo klonowanie stanu\n",
    "state.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rozegraj grę z gotową implementacją MCTS,  \n",
    "# Ty zawsze zaczynasz (masz przewagę, rozpoczynający optymalny gracz zawsze powinien wygrać)\n",
    "from math import sqrt\n",
    "from open_spiel.python.algorithms import mcts\n",
    "\n",
    "game = pyspiel.load_game(\"connect_four\")\n",
    "state = game.new_initial_state()\n",
    "\n",
    "# Drugi argument MCTSBot to paramter c w UCB, trzeci argument to liczba symulacji, \n",
    "# im mniej symulacji tym większa szansa, że bot wykona słaby ruch, \n",
    "# sprawdz czy jesteś w stanie wygrać z botem o 10, 100, 1000 symulacjach na ruch\n",
    "bot = mcts.MCTSBot(game, sqrt(2), 1000, mcts.RandomRolloutEvaluator())\n",
    "\n",
    "while not state.is_terminal():\n",
    "    if(state.current_player() == 1):\n",
    "        a = bot.step(state)\n",
    "    else:\n",
    "        print('Tura gracza:', state.current_player())\n",
    "        print('Ruch:', state.move_number())\n",
    "        print('Stan:')\n",
    "        print(state.observation_string())\n",
    "        print('Dopuszczalne akcje:', state.legal_actions())\n",
    "        a = int(input())\n",
    "    state.apply_action(a)\n",
    "print('Koniec gry, wyniki:', state.player_reward(0), ':', state.player_reward(1)) # albo state.rewards()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interface agenta\n",
    "\n",
    "Implementacja zadania będzie się opierać o klasę `OpenSpielAgent` przedstawioną poniżej. Powinna ona implementować jedną metodę `step`, która będzie na podstawie otrzymanego stanu zwracać akcję.\n",
    "\n",
    "Poniżej znajdziesz też funkcję `play_games`, która korzysta z opisanego wyżej obiektu agenta w celu jego ewaluacji (podobną będziemy używać my podczas sprawdzania wszych rozwiązań). Możecie ją modyfikować jeśli widzicie taką potrzebę. Dopisaliśmy do tej funkcji rysowanie wykresu sumy nagród z minionych epizodów (gier), który uaktualnia się co ich zadaną liczbę."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenSpielAgent:\n",
    "    def __init__(self, game):\n",
    "        self.game = game\n",
    "        \n",
    "    def step(self, state):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def plot_rewards(rewards):\n",
    "    plt.figure(figsize=(14,6))\n",
    "    for r in rewards:\n",
    "        plt.plot(r)\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    \n",
    "def play_game(game, agents, episodes, plot=False, plot_interval=1000):\n",
    "    assert len(agents) == game.num_players()\n",
    "    rewards_history = [[0] for _ in agents]\n",
    "    for episode in range(episodes):\n",
    "        state = game.new_initial_state()\n",
    "        while not state.is_terminal():\n",
    "            \n",
    "            # Wyznaczmy aktualnego gracza (to, który agent jest, którym graczem zależy od numeru epizodu)\n",
    "            p = (state.current_player() + episode) % game.num_players()\n",
    "            \n",
    "            # Zapytajmy agenta o akcje dla aktualnego stanu \n",
    "            # (agent dostaje kopię stanu, by nie mógł manipulować prawdziwym przebiegiem rozgrywki)\n",
    "            a = agents[p].step(state.clone())\n",
    "            \n",
    "            # Wykonajmy akcje\n",
    "            # (jeśli agent zwróci niedozwoloną akcję, zostanie wykonana losowa akcja)\n",
    "            try:\n",
    "                state.apply_action(a)\n",
    "            except:\n",
    "                print(\"Agent selected inlegal action, using random action instead\") \n",
    "                state.apply_action(np.random.choice(state.legal_actions()))\n",
    "            \n",
    "        for i in range(game.num_players()):\n",
    "            p = (i + episode) % game.num_players()\n",
    "            rewards_history[i].append(rewards_history[i][-1] + state.player_reward(p))\n",
    "        \n",
    "        # Wyświetl na wykresie sumę nagród z minionych epizodów\n",
    "        if plot and episode % plot_interval == 0:\n",
    "            plot_rewards(rewards_history)\n",
    "    return rewards_history    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstracja na przykładzie losowego agenta vs agenta używającego MCTS\n",
    "\n",
    "class RandomAgent(OpenSpielAgent):\n",
    "    def __init__(self, game):\n",
    "        super().__init__(game)\n",
    "        \n",
    "    def step(self, state):\n",
    "        return np.random.choice(state.legal_actions())\n",
    "\n",
    "    \n",
    "game = pyspiel.load_game(\"connect_four\")\n",
    "random_agent = RandomAgent(game)\n",
    "mcts_agent = mcts.MCTSBot(game, sqrt(2), 100, mcts.RandomRolloutEvaluator())\n",
    "rewards = play_game(game, (random_agent, mcts_agent), 10, plot=True, plot_interval=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zad 2 - Monte Carlo Tree Search (20pkt)\n",
    "\n",
    "W komórkach poniżej zaimplementuj Agenta używającego Monte Carlo Tree Search dla środowiska `Connect Four`. Użyj losowej polityki rolloutu oraz polityki drzewa opartej o UCB. \n",
    "\n",
    "Odpowiedz na pytania (bonusowe):\n",
    "- Ile stanów posiada środowisko `Connect Four` (ile wierzchołków ma drzewo stanów, możesz policzyć za pomocą komutera).\n",
    "- Czy widzisz jakiś sprytny sposób na mniejszenie przestrzeni stanów dla gry `Connect Four`? (nie implemnetuj jej).\n",
    "\n",
    "Uwagi:\n",
    "- Zadbaj o to by wybór ruch przez Twojego agenta nie trwał dłużej niż 1-2 sekundy.\n",
    "- Postaraj się dobrać i ustawić w `__init__` takie parametry by Twój Agent uczył się i działał jak najlepiej. Są one pod Twoją kontrolą w klasie, więc możesz je odpowiednio kontrolować w trakcie uczenia.\n",
    "- Agent powinen wygrywać albo remisować większość gier, które rozpoczyna przeciwko innemu silnemu graczowi (np. gotowemu botowi MCTS).\n",
    "\n",
    "#### Uwaga: nie zmieniaj nazwy klasy `MCTSAgent`, ani pozycyjnych (tych bez wartości domyślnych) argumentów jej metod, możesz dopisywać argumenty z wartościami domyślnymi oraz oczywiście pomocnicze meotdy do klasy. Nie dopisuj do komórki z klasą innego kodu."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Odpowiedzi: Miejsce na Twoje odpowiedzi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTSAgent(Agent):\n",
    "    def __init__(self, game):\n",
    "        super().__init__(game)\n",
    "        # Zainicjalizuj tutaj swojego agenta\n",
    "    \n",
    "    def step(self, state):\n",
    "        raise NotImplementedError()\n",
    "        # Miejsce na Twoją implementację"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testowania agenta\n",
    "game = pyspiel.load_game(\"connect_four\")\n",
    "agent = MCTSAgent(game)\n",
    "mcts_agent = mcts.MCTSBot(game, sqrt(2), 100, mcts.RandomRolloutEvaluator())\n",
    "play_game(game, (agent, mcts_agent), 100, plot=True, plot_interval=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
